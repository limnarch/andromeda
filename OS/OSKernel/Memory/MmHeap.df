//
// Implements the free-list allocator for the kernel pool.
//

#include "<df>/dragonfruit.h"

#include "<inc>/HALLog.h"
#include "<inc>/HALCPU.h"
#include "<inc>/HALMap.h"
#include "<inc>/HALDebug.h"

#include "<inc>/Kernel.h"

#include "<inc>/Executive.h"

#include "<inc>/Memory.h"

#include "<ll>/OSDLL/OS.h"

#include "MmInternal.h"

// heap allocation functions. should be called only through MmAllocWithTag and
// MmFree. manages free-list allocations within pages.

// we allocate in multiple "levels" in the interest of stability during tight
// memory situations. the levels are as follows:
//
// - NORMAL
// - MUSTSUCCEEDL2
// - MUSTSUCCEED
//
// the allocation request will begin by searching the NORMAL free lists for a
// block to return or split. if they are empty, and the MUSTSUCCEEDL2 or
// MUSTSUCCEED flag has been specified, it will search those lists in that
// order until it either finds an appropriate block or reaches the last level
// it may allocate from. if it reaches this level without having found a
// block, it will allocate a page from the corresponding MmPageAlloc priority
// and split it into blocks to place on that level's free lists.

const MMHEAPBLOCKBUCKETS (PAGESIZE MMHEAPBLOCKMINSIZE /)

table MmHeapNormalListHeads[(MMHEAPBLOCKBUCKETS 1 +)]
table MmHeapMSL2ListHeads[(MMHEAPBLOCKBUCKETS 1 +)]
table MmHeapMustSucceedListHeads[(MMHEAPBLOCKBUCKETS 1 +)]

table MmHeapPagedListHeads[(MMHEAPBLOCKBUCKETS 1 +)]

table MmHeapListHeads
	pointerof MmHeapNormalListHeads
	pointerof MmHeapMSL2ListHeads
	pointerof MmHeapMustSucceedListHeads
	pointerof MmHeapPagedListHeads
endtable

table MmHeapLevelPriorities
	0
	MUSTSUCCEEDL2
	MUSTSUCCEED
	0
endtable

var MmNonpagedHeapBytesUsedInternally 0
public MmNonpagedHeapBytesUsedInternally

var MmNonpagedHeapBytesUsedPeak 0
public MmNonpagedHeapBytesUsedPeak

var MmNonpagedHeapBytesUsedExternally 0
public MmNonpagedHeapBytesUsedExternally

var MmPagedHeapBytesUsedInternally 0
public MmPagedHeapBytesUsedInternally

var MmPagedHeapBytesUsedPeak 0
public MmPagedHeapBytesUsedPeak

var MmPagedHeapBytesUsedExternally 0
public MmPagedHeapBytesUsedExternally

buffer MiPagedHeapMutex KeMutex_SIZEOF

fn MiHeapInit { -- }
	fnsection "INIT$text"

	"MiPagedHeapMutex" // name
	KERNELMODE // mode
	MiPagedHeapMutex
	KeMutexInitialize
end

fn MiHeapAlloc { bytes tag flags -- ptr ok }
	auto maxlevel

	auto ipl

	if (flags@ PAGED &)
		IPLAPC KeIPLRaise ipl!

		KERNELMODE // waitmode
		0 // alertable
		OSWAIT_TIMEOUTINFINITE // timeout
		MiPagedHeapMutex // object
		KeThreadWaitForObject drop

		-1 // ipl
		bytes@ // bytes
		tag@ // tag
		MMHEAPPAGED // level
		flags@ // flags
		MiHeapAllocateLevel ok! ptr!

		0 // abandon
		MiPagedHeapMutex // mutex
		KeMutexRelease drop

		ipl@ KeIPLLower

		return
	end

	if (flags@ MUSTSUCCEED &)
		MMHEAPMS maxlevel!
	end elseif (flags@ MUSTSUCCEEDL2 &)
		MMHEAPMSL2 maxlevel!
	end else
		MMHEAPNORMAL maxlevel!
	end

	auto level
	MMHEAPNORMAL level!

	while (level@ maxlevel@ <=)
		IPLDPC KeIPLRaise ipl!

		if (level@ maxlevel@ ==)
			ipl@ // ipl
			bytes@ // bytes
			tag@ // tag
			level@ // level
			flags@ // flags
			MiHeapAllocateLevel ok! ptr!
		end else
			ipl@ // ipl
			bytes@ // bytes
			tag@ // tag
			level@ // level
			[level@]MmHeapLevelPriorities@ // flags
			MiHeapAllocateLevel ok! ptr!
		end

		ipl@ KeIPLLower

		if (ok@ ~~)
			return
		end

		1 level +=
	end
end

fn MiHeapAllocateLevel { ipl bytes tag level flags -- ptr ok }
	// called at IPLDPC if nonpaged, or with MiPagedHeapMutex held if paged.

	MiAllocatedHeapBlock_SIZEOF bytes +=

	auto n

	auto bucketindex
	bytes@ MMHEAPBLOCKSIZEMASK + MMHEAPBLOCKSIZESHIFT >> bucketindex!

	bucketindex@ MMHEAPBLOCKSIZESHIFT << bytes!

	auto listheads
	[level@]MmHeapListHeads@ listheads!

	listheads@ bucketindex@ 2 << + @ ptr!

	if (ptr@)
		// there's a properly sized block already

		if (ptr@ MiHeapBlock_Magic + gi MMHEAPFREEMAGIC ~=)
			ptr@ MiHeapBlock_Magic + gi "MiHeapAllocate: bad magic 0x%04x\n" KeCrash
		end

		if (ptr@ MiHeapBlock_BucketIndex + gb bucketindex@ ~=)
			bucketindex@ ptr@ MiHeapBlock_BucketIndex + gb "MiHeapAllocate: bad bucketindex %d (expected %d)\n" KeCrash
		end

		// set magic to indicate allocated
		MMHEAPSWIFTMAGIC level@ | ptr@ MiHeapBlock_Magic + si

		// unlink from free list
		ptr@ MiHeapBlock_NextFree + @ n!

		if (n@)
			0 n@ MiHeapBlock_PrevFree + !
		end

		n@ listheads@ bucketindex@ 2 << + !

		if (flags@ PAGED &)
			bytes@ MmPagedHeapBytesUsedInternally +=

			if (MmPagedHeapBytesUsedInternally@ MmPagedHeapBytesUsedPeak@ >)
				MmPagedHeapBytesUsedInternally@ MmPagedHeapBytesUsedPeak!
			end
		end else
			bytes@ MmNonpagedHeapBytesUsedInternally +=

			if (MmNonpagedHeapBytesUsedInternally@ MmNonpagedHeapBytesUsedPeak@ >)
				MmNonpagedHeapBytesUsedInternally@ MmNonpagedHeapBytesUsedPeak!
			end
		end

		tag@ ptr@ MiAllocatedHeapBlock_Tag + !

		// return block
		MiAllocatedHeapBlock_SIZEOF ptr +=

		0 ok!

		return
	end

	// try to find a bigger block that can be split

	auto newbucketindex

	auto newblock

	auto i
	MMHEAPBLOCKBUCKETS i!

	while (i@ bucketindex@ >)
		listheads@ i@ 2 << + @ ptr!

		if (ptr@)
			// found one to split, do that

			if (ptr@ MiHeapBlock_Magic + gi MMHEAPFREEMAGIC ~=)
				ptr@ MiHeapBlock_Magic + gi "MiHeapAllocate: bad magic 2 0x%04x\n" KeCrash
			end

			if (ptr@ MiHeapBlock_BucketIndex + gb i@ ~=)
				i@ ptr@ MiHeapBlock_BucketIndex + gb "MiHeapAllocate: bad bucketindex 2 %d (expected %d)\n" KeCrash
			end

			// remove from free list
			ptr@ MiHeapBlock_NextFree + @ n!

			if (n@)
				0 n@ MiHeapBlock_PrevFree + !
			end

			n@ listheads@ i@ 2 << + !

			// set new bucketindex in old block header
			// already has correct lastbucketindex
			bucketindex@ ptr@ MiHeapBlock_BucketIndex + sb
			MMHEAPSWIFTMAGIC level@ | ptr@ MiHeapBlock_Magic + si

			// calculate index of bucket to place split block in
			i@ bucketindex@ - newbucketindex!

			ptr@ bytes@ + newblock!

			// create new block header
			bucketindex@ newblock@ MiHeapBlock_LastBucketIndex + sb
			newbucketindex@ newblock@ MiHeapBlock_BucketIndex + sb
			MMHEAPFREEMAGIC newblock@ MiHeapBlock_Magic + si

			// update next block to point to new block unless it is page
			// -aligned which means theres no next block
			newblock@ newbucketindex@ MMHEAPBLOCKSIZESHIFT << + n!

			if (n@ PAGEOFFSETMASK &)
				newbucketindex@ n@ MiHeapBlock_LastBucketIndex + sb
			end

			// insert in new bucket list
			listheads@ newbucketindex@ 2 << + @ n!

			if (n@)
				newblock@ n@ MiHeapBlock_PrevFree + !
			end

			n@ newblock@ MiHeapBlock_NextFree + !
			0 newblock@ MiHeapBlock_PrevFree + !

			newblock@ listheads@ newbucketindex@ 2 << + !

			if (flags@ PAGED &)
				bytes@ MmPagedHeapBytesUsedInternally +=

				if (MmPagedHeapBytesUsedInternally@ MmPagedHeapBytesUsedPeak@ >)
					MmPagedHeapBytesUsedInternally@ MmPagedHeapBytesUsedPeak!
				end
			end else
				bytes@ MmNonpagedHeapBytesUsedInternally +=

				if (MmNonpagedHeapBytesUsedInternally@ MmNonpagedHeapBytesUsedPeak@ >)
					MmNonpagedHeapBytesUsedInternally@ MmNonpagedHeapBytesUsedPeak!
				end
			end

			// set tag
			tag@ ptr@ MiAllocatedHeapBlock_Tag + !

			// return block
			MiAllocatedHeapBlock_SIZEOF ptr +=

			0 ok!

			return
		end

		1 i -=
	end

	// no sufficient block! allocate a page.
	// store pfdbe in n.

	if (flags@ PAGED &)
		PAGESIZE // bytes
		'PgHp' // tag
		flags@ POOLEXP | // flags
		MiPagedPoolAllocPages ok! ptr!

		if (ok@)
			return
		end
	end else
		if (flags@ CANBLOCK &)
			ipl@ KeIPLLower
		end

		PAGESIZE // bytes
		'NpHp' // tag
		flags@ POOLEXP | // flags
		MiNonpagedPoolAllocPages ok! ptr! n!

		if (flags@ CANBLOCK &)
			IPLDPC KeIPLRaise drop
		end

		if (ok@)
			return
		end

		// initialize the pfdbe
		level@ n@ MiPageFrameEntryHeap_Level + !
		0 n@ MiPageFrameEntryHeap_Permanent + !
		//n@ MiPoolPageInsert
	end

	// set bucketindex in our block header
	0 ptr@ MiHeapBlock_LastBucketIndex + sb
	bucketindex@ ptr@ MiHeapBlock_BucketIndex + sb
	MMHEAPSWIFTMAGIC level@ | ptr@ MiHeapBlock_Magic + si

	// calculate index of bucket to place split block in
	PAGESIZE MMHEAPBLOCKSIZESHIFT >> bucketindex@ - newbucketindex!

	ptr@ bytes@ + newblock!

	// create free block header
	bucketindex@ newblock@ MiHeapBlock_LastBucketIndex + sb
	newbucketindex@ newblock@ MiHeapBlock_BucketIndex + sb
	MMHEAPFREEMAGIC newblock@ MiHeapBlock_Magic + si

	// insert in free list
	listheads@ newbucketindex@ 2 << + @ n!

	if (n@)
		newblock@ n@ MiHeapBlock_PrevFree + !
	end

	n@ newblock@ MiHeapBlock_NextFree + !
	0 newblock@ MiHeapBlock_PrevFree + !

	newblock@ listheads@ newbucketindex@ 2 << + !

	if (flags@ PAGED &)
		PAGESIZE MmPagedHeapBytesUsedExternally +=
		bytes@ MmPagedHeapBytesUsedInternally +=

		if (MmPagedHeapBytesUsedInternally@ MmPagedHeapBytesUsedPeak@ >)
			MmPagedHeapBytesUsedInternally@ MmPagedHeapBytesUsedPeak!
		end
	end else
		PAGESIZE MmNonpagedHeapBytesUsedExternally +=
		bytes@ MmNonpagedHeapBytesUsedInternally +=

		if (MmNonpagedHeapBytesUsedInternally@ MmNonpagedHeapBytesUsedPeak@ >)
			MmNonpagedHeapBytesUsedInternally@ MmNonpagedHeapBytesUsedPeak!
		end
	end

	// set tag
	tag@ ptr@ MiAllocatedHeapBlock_Tag + !

	MiAllocatedHeapBlock_SIZEOF ptr +=

	0 ok!
end

fn MiHeapFree { ptr -- }
	MiAllocatedHeapBlock_SIZEOF ptr -=

	// check if the block has free blocks physically to its left or right.
	// if so, merge.

	// then, place the resulting block on the relevant free list.

	auto bucketindex
	auto lastbucketindex
	auto nb
	auto next
	auto prev

	ptr@ MiHeapBlock_BucketIndex + gb bucketindex!

	if (ptr@ MiHeapBlock_Magic + gi 15 ~ & MMHEAPSWIFTMAGIC ~=)
		ptr@ MiHeapBlock_Magic + gi "MiHeapFree: bad magic %x\n" KeCrash
	end

	auto level
	ptr@ MiHeapBlock_Magic + gi 15 & level!

	auto listheads
	[level@]MmHeapListHeads@ listheads!

	auto ipl

	if (level@ MMHEAPPAGED ~=)
		IPLDPC KeIPLRaise ipl!

		bucketindex@ MMHEAPBLOCKSIZESHIFT <<
		MmNonpagedHeapBytesUsedInternally -=
	end else
		KERNELMODE // waitmode
		0 // alertable
		OSWAIT_TIMEOUTINFINITE // timeout
		MiPagedHeapMutex // object
		KeThreadWaitForObject drop

		bucketindex@ MMHEAPBLOCKSIZESHIFT <<
		MmPagedHeapBytesUsedInternally -=
	end

	if (ptr@ PAGEOFFSETMASK &)
		// we're not at the start of the page, check left

		ptr@ MiHeapBlock_LastBucketIndex + gb lastbucketindex!

		ptr@ lastbucketindex@ MMHEAPBLOCKSIZESHIFT << - nb!

		if (nb@ MiHeapBlock_BucketIndex + gb lastbucketindex@ ~=)
			lastbucketindex@ nb@ MiHeapBlock_BucketIndex + gb "MiHeapFree: bad bucketindex %d (expected %d)\n" KeCrash
		end

		if (nb@ MiHeapBlock_Magic + gi MMHEAPFREEMAGIC ==)
			// free! merge left.

			// remove from old free list
			nb@ MiHeapBlock_PrevFree + @ prev!

			nb@ MiHeapBlock_NextFree + @ next!

			if (prev@)
				next@ prev@ MiHeapBlock_NextFree + !
			end else
				next@ listheads@ lastbucketindex@ 2 << + !
			end

			if (next@)
				prev@ next@ MiHeapBlock_PrevFree + !
			end

			// turn block into bigger block
			lastbucketindex@ bucketindex +=

			bucketindex@ nb@ MiHeapBlock_BucketIndex + sb

			// invalidate magic number of old block
			0 ptr@ MiHeapBlock_Magic + si

			// set block pointer to last block
			nb@ ptr!

			// set right block lastbucketindex to our new one.
			// we might be about to merge with it but its easier to just do
			// this anyway.

			ptr@ bucketindex@ MMHEAPBLOCKSIZESHIFT << + nb!

			if (nb@ PAGEOFFSETMASK &)
				bucketindex@ nb@ MiHeapBlock_LastBucketIndex + sb
			end
		end
	end

	ptr@ bucketindex@ MMHEAPBLOCKSIZESHIFT << + nb!

	if (nb@ PAGEOFFSETMASK &)
		// next block exists, see if it can be merged

		nb@ MiHeapBlock_BucketIndex + gb lastbucketindex!

		if (nb@ MiHeapBlock_Magic + gi MMHEAPFREEMAGIC ==)
			// free! merge right

			// remove from old free list
			nb@ MiHeapBlock_PrevFree + @ prev!

			nb@ MiHeapBlock_NextFree + @ next!

			if (prev@)
				next@ prev@ MiHeapBlock_NextFree + !
			end else
				next@ listheads@ lastbucketindex@ 2 << + !
			end

			if (next@)
				prev@ next@ MiHeapBlock_PrevFree + !
			end

			// turn block into bigger block
			lastbucketindex@ bucketindex +=

			bucketindex@ ptr@ MiHeapBlock_BucketIndex + sb

			// invalidate magic number of old block
			0 nb@ MiHeapBlock_Magic + si

			// set right block lastbucketindex to our new one.

			ptr@ bucketindex@ MMHEAPBLOCKSIZESHIFT << + nb!

			if (nb@ PAGEOFFSETMASK &)
				bucketindex@ nb@ MiHeapBlock_LastBucketIndex + sb
			end
		end
	end

	if (bucketindex@ MMHEAPBLOCKBUCKETS ==)
		// free this heap page.

		if (level@ MMHEAPPAGED ==)
			PAGESIZE MmPagedHeapBytesUsedExternally -=

			0 // abandon
			MiPagedHeapMutex // mutex
			KeMutexRelease drop

			1 // noaccount
			ptr@ // ptr
			MiPagedPoolFreePages

			return
		end else
			PAGESIZE MmNonpagedHeapBytesUsedExternally -=

			1 // noaccount
			ptr@ // ptr
			MiNonpagedPoolFreePages

			ipl@ KeIPLLower

			return
		end
	end

	// set free magic
	MMHEAPFREEMAGIC ptr@ MiHeapBlock_Magic + si

	// put on relevant free list
	listheads@ bucketindex@ 2 << + @ nb!

	if (nb@)
		ptr@ nb@ MiHeapBlock_PrevFree + !
	end

	nb@ ptr@ MiHeapBlock_NextFree + !
	0 ptr@ MiHeapBlock_PrevFree + !

	ptr@ listheads@ bucketindex@ 2 << + !

	if (level@ MMHEAPPAGED ==)
		0 // abandon
		MiPagedHeapMutex // mutex
		KeMutexRelease drop
	end else
		ipl@ KeIPLLower
	end
end

fn MmHeapChargeBytesGet { bytes -- charge }
	MiAllocatedHeapBlock_SIZEOF bytes +=

	auto bucketindex
	bytes@ MMHEAPBLOCKSIZEMASK + MMHEAPBLOCKSIZESHIFT >> bucketindex!

	bucketindex@ MMHEAPBLOCKSIZESHIFT << charge!
end

fn MmHeapChargeGet { block -- charge }
	MiAllocatedHeapBlock_SIZEOF block -=

	if (block@ MiAllocatedHeapBlock_Magic + gi 15 ~ & MMHEAPSWIFTMAGIC ~=)
		block@ MiAllocatedHeapBlock_Magic + gi
		block@ "MmHeapChargeGet: ptr 0x%08x had bad magic 0x%08x\n" KeCrash
	end

	block@ MiAllocatedHeapBlock_BucketIndex + gb MMHEAPBLOCKSIZESHIFT << charge!
end

extern MmHeapPrintTag { tag -- }

fn MmHeapDumpPage { tag page -- usage }
	0 usage!

	while (1)
		if (page@ MiHeapBlock_Magic + gi MMHEAPSWIFTMAGIC ==)
			if (page@ MiAllocatedHeapBlock_Tag + @ tag@ == tag@ ~~ ||)
				page@ MiAllocatedHeapBlock_Tag + @ MmHeapPrintTag
				page@ MiHeapBlock_BucketIndex + gb MMHEAPBLOCKSIZESHIFT <<
				page@ MiAllocatedHeapBlock_SIZEOF +
				" %08x (%d bytes)\n" Printf

				page@ MiHeapBlock_BucketIndex + gb MMHEAPBLOCKSIZESHIFT << usage +=
			end
		end

		page@ MiHeapBlock_BucketIndex + gb MMHEAPBLOCKSIZESHIFT << page +=

		if (page@ PAGEOFFSETMASK & ~~)
			return
		end
	end
end