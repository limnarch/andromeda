//
// Implements the kernel pool allocation entrypoints, and the page-aligned
// allocator.
//

#include "<df>/dragonfruit.h"

#include "<inc>/HALLog.h"
#include "<inc>/HALCPU.h"
#include "<inc>/HALMap.h"
#include "<inc>/HALDebug.h"

#include "<inc>/Kernel.h"

#include "<inc>/Executive.h"

#include "<inc>/Memory.h"

#include "<inc>/IO.h"

#include "<inc>/Security.h"

#include "<inc>/Process.h"

#include "<ll>/OSDLL/OSStatus.h"

#include "../../Common/Common.h"

#include "MmInternal.h"

buffer MiPoolSpaceBitmapHeader ComBitmapHeader_SIZEOF
public MiPoolSpaceBitmapHeader

buffer MiPoolSpacePagedEndBitmapHeader ComBitmapHeader_SIZEOF
public MiPoolSpacePagedEndBitmapHeader

buffer MiPoolSpaceBitmap (POOLPAGES 7 + 3 >>)
buffer MiPoolSpacePagedEndBitmap (POOLPAGES 7 + 3 >>)

var MmNonpagedPoolBytesUsed 0
public MmNonpagedPoolBytesUsed

var MmPagedPoolBytesUsed 0
public MmPagedPoolBytesUsed

var MmNonpagedPoolBytesPeak 0
public MmNonpagedPoolBytesPeak

var MmPagedPoolBytesPeak 0
public MmPagedPoolBytesPeak

var MiPoolPageListHead 0

// The heap is managed within a 32MB region of kernel space called
// "pool space". Allocations less than a page size are handled by a slab
// allocator within pool pages. Allocations of a page size or more are handled
// by directly allocating pool pages.

extern MiHeapInit { -- }

fn MiPoolInit { -- }
	fnsection "INIT$text"

	POOLPAGES // sizeinbits
	MiPoolSpaceBitmap // data	
	MiPoolSpaceBitmapHeader // header
	ComBitmapInitialize

	POOLPAGES // sizeinbits
	MiPoolSpacePagedEndBitmap // data	
	MiPoolSpacePagedEndBitmapHeader // header
	ComBitmapInitialize

	MiHeapInit
end

fn MmHeapPrintTag { tag -- }
	auto shf
	32 shf!

	while (shf@)
		8 shf -=

		auto c
		tag@ shf@ >> 0xFF & c!

		if (c@ 0x80 & ~~ c@ 0x20 >= &&)
			c@ Putc
		end else
			'!' Putc
		end
	end
end

fn MmHeapCheck { -- }

end

fn MmHeapDumpBlockInfo { block -- }

end

extern MmHeapDumpPage { tag page -- usage }

fn MmPoolDump { tag -- usage }
	auto pfdbe
	MiPoolPageListHead@ pfdbe!

	0 usage!

	while (pfdbe@)
		if (pfdbe@ MiPageFrameEntryPool_Level + @ -1 ==)
			// page-aligned

			if (tag@ ~~ pfdbe@ MiPageFrameEntryPool_Tag + @ tag@ == ||)
				pfdbe@ MiPageFrameEntryPool_Tag + @ MmHeapPrintTag
				pfdbe@ MiPageFrameEntryPool_ByteSize + @
				pfdbe@ MiPageFrameEntryPool_VirtualAddress + @
				" %08x (%d bytes)\n" Printf

				pfdbe@ MiPageFrameEntryPool_ByteSize + @ usage +=
			end
		end else
			// heap

			tag@ // tag
			pfdbe@ MiPageFrameEntryPool_VirtualAddress + @
			MmHeapDumpPage usage +=
		end

		pfdbe@ MiPageFrameEntryPool_PoolListNext + @ pfdbe!
	end
end

fn MmAllocWithTag { bytes tag flags -- ptr ok }
	if (DEBUGCHECKS)
		if (MiInited@ ~~)
			"MmAllocWithTag: used before MmInit called\n" KeCrash
		end

		if (KeIPLCurrentGet IPLDPC >)
			"MmAllocWithTag: ipl > IPLDPC\n" KeCrash
		end
	end

	if (bytes@ ~~)
		STATUS_INVALID_ARGUMENT ok!
	end

	if (flags@ 0 ==)
		// can't block, so give this nonpaged allocation a small leg up
		POOLALLOC flags!
	end

	// round up to nearest long
	bytes@ 3 + 3 ~ & bytes!

	if (bytes@ MiAllocatedHeapBlock_SIZEOF + PAGESIZE MMHEAPBLOCKMINSIZE - >=)
		if (flags@ PAGED &)
			bytes@ // bytes
			tag@ // tag
			flags@ // flags
			MiPagedPoolAllocPages ok! ptr!
		end else
			bytes@ // bytes
			tag@ // tag
			flags@ // flags
			MiNonpagedPoolAllocPages ok! ptr! drop
		end

		if (DEBUGCHECKS)
			if (ok@)
				if (flags@ CANBLOCK &)
					"MmAllocWithTag: page-aligned CANBLOCK allocation failed\n" KeCrash
				end
			end
		end

		return
	end

	bytes@ // bytes
	tag@ // tag
	flags@ // flags
	MiHeapAlloc ok! ptr!

	if (DEBUGCHECKS)
		if (ok@)
			if (flags@ CANBLOCK &)
				"MmAllocWithTag: CANBLOCK allocation failed\n" KeCrash
			end
		end
	end
end

fn MmFree { ptr -- }
	if (DEBUGCHECKS)
		if (MiInited@ ~~)
			"MmFree: used before MmInit called\n" KeCrash
		end

		if (ptr@ -1 ==)
			"MmFree: tried to free -1 pointer\n" KeCrash
		end

		if (KeIPLCurrentGet IPLDPC >)
			"MmFree: ipl > IPLDPC\n" KeCrash
		end
	end

	if (ptr@ MMLOWESTSYSTEMADDRESS <)
		ptr@ "MmFree: tried to free null pointer (%x)\n" KeCrash
	end

	if (ptr@ PAGEOFFSETMASK & ~~)
		// page aligned.
		// determine if the given block is in paged or nonpaged pool.

		auto pteaddr
		ptr@ // vaddr
		MmVirtualtoPTEAddress pteaddr!

		auto ipl
		IPLDPC KeIPLRaise ipl!

		auto ok
		auto phyaddr
		pteaddr@ // pteaddr
		MiPTEInterpret ok! drop phyaddr!

		if (ok@)
			ipl@ KeIPLLower

			// only paged pool can be non-resident.
			// and also mistaken frees.

			if (DEBUGCHECKS)
				if (ptr@ POOLSPACE <)
					ptr@ "MmFree: ptr 0x%08x < POOLSPACE\n" KeCrash
				end

				if (ptr@ POOLSPACE POOLSIZE + >=)
					ptr@ "MmFree: ptr 0x%08x beyond pool space\n" KeCrash
				end
			end

			0 // noaccount
			ptr@ // ptr
			MiPagedPoolFreePages
		end else
			// maybe paged or nonpaged. check PFDBE.

			auto pfdbe
			phyaddr@ PAGESHIFT >> MiPageFrameEntry_SIZEOF * MiPageFrameDatabase@ + pfdbe!

			if (pfdbe@ MiPageFrameEntryPool_ZeroIfNonPaged + @ 0 ==)
				ipl@ KeIPLLower

				// nonpaged.

				0 // noaccount
				ptr@ // ptr
				MiNonpagedPoolFreePages
			end else
				ipl@ KeIPLLower

				// paged.

				0 // noaccount
				ptr@ // ptr
				MiPagedPoolFreePages
			end
		end

		return
	end

	ptr@ MiHeapFree
end

var MiPoolPageHint 0

var MiPoolSpaceUsed 0
public MiPoolSpaceUsed

fn MiPoolSpaceReserve { pri pagesneeded -- offset ok }
	auto bmpheader
	MiPoolSpaceBitmapHeader bmpheader!

	while (1)
		auto ipl
		IPLDPC KeIPLRaise ipl!

		MiPoolPageHint@ // hint
		pagesneeded@ // runlength
		bmpheader@ // header
		ComBitmapFindRun ok! offset!

		if (ok@)
			if (pri@ PAGED CANBLOCK | & ~~
				DEBUGCHECKS ||)

				ipl@ KeIPLLower

				// just crash right away if can't block or checked build

				bmpheader@ ComBitmapDump
				ok@ pagesneeded@ "\n\n%d (%i)\n" Printf

				POOLPAGES
				MiPoolSpaceUsed@
				"used: %d / %d\n" Printf

				MmPagedPoolBytesUsed@ PAGESHIFT >>
				"pagedpool: %d\n" Printf

				"out of poolspace\n" KeCrash

				return
			end

			// wait aimlessly for 100 milliseconds and repeat the request.
			// this is a last resort for stability purposes on release builds.
			// we should never ever get to this point; POOLSPACE is not
			// designed to be a scarce resource like physical and virtual
			// memory are, since the chicken and egg problem has to have a
			// first chicken somewhere. this is not a problem during normal
			// operation since pool quota prevents user processes from
			// consuming this much POOLSPACE, and its honestly difficult to
			// run out of POOLSPACE without running out of other resources
			// first (which can be coped with) anyway.

			ipl@ KeIPLLower

			100 // ms
			KERNELMODE // waitmode
			0 // alertable
			KeThreadSleep drop

			continue
		end

		offset@ MiPoolPageHint!

		pagesneeded@ // runlength
		offset@ // index
		bmpheader@ // header
		ComBitmapSetBits

		pagesneeded@ MiPoolSpaceUsed +=

		ipl@ KeIPLLower

		break
	end
end

fn MiPoolSpaceRelease { pages offset -- }
	auto ipl
	IPLDPC KeIPLRaise ipl!

	pages@ // runlength
	offset@ // index
	MiPoolSpaceBitmapHeader // header
	ComBitmapClearBits

	pages@ MiPoolSpaceUsed -=

	ipl@ KeIPLLower
end

fn MiPagedPoolAllocPages { bytes tag flags -- realva ok }
	// this is a kernel mapping, so reserve POOLSPACE and use that as the
	// startva. it must be done like this because kernel page tables are
	// necessarily not dynamic.

	auto pages
	bytes@ PAGEOFFSETMASK + PAGESHIFT >> pages!

	pages@ PAGESHIFT << // charge
	MmQuotaSystem // quotablock
	MmQuotaBlockChargeVM ok!

	if (ok@)
		return
	end

	auto startva
	CANBLOCK // pri
	pages@ // pages
	MiPoolSpaceReserve ok! startva!

	if (ok@)
		pages@ PAGESHIFT << // charge
		MmQuotaSystem // quotablock
		MmQuotaBlockUnchargeVM

		return
	end

	// set the bit for the final page in this allocation in the final page
	// bitmap so we can count bits to know how long it is when we go to free.

	auto ipl
	IPLDPC KeIPLRaise ipl!

	1 // runlength
	startva@ pages@ 1 - + // index
	MiPoolSpacePagedEndBitmapHeader // header
	ComBitmapSetBits

	if (flags@ POOLEXP & ~~)
		pages@ PAGESHIFT << MmPagedPoolBytesUsed +=

		if (MmPagedPoolBytesUsed@ MmPagedPoolBytesPeak@ >)
			MmPagedPoolBytesUsed@ MmPagedPoolBytesPeak!
		end
	end

	ipl@ KeIPLLower

	// calculate the vaddr from the index MiPoolSpaceReserve gave us.

	startva@ PAGESHIFT << POOLSPACE + startva!
	startva@ realva!

	// initialize the PTEs in that region of POOLSPACE as kernel demand-
	// zero. this avoids having to allocate a VAD for private kernel
	// mappings, which makes them much cheaper. we can't do this for user-
	// space because userspace page tables are dynamically created and
	// deleted as pages are faulted in and removed which necessitates a
	// more permanent place for information. also, VADs are going to be
	// placed in paged pool, which will cause a dependency cycle if we use
	// them for private kernel mappings.

	auto pteaddr
	0 pteaddr!

	while (pages@)
		if (startva@ PERPAGETABLEOFFSETMASK & ~~ pteaddr@ ~~ ||)
			startva@ // vaddr
			MmVirtualtoPTEAddress pteaddr!
		end

		PTE_KERNEL_DEMANDZERO pteaddr@!

		PTESIZE pteaddr +=
		PAGESIZE startva +=
		1 pages -=
	end

	// TODO Tag
end

fn MiPagedPoolFreePages { noaccount vaddr -- }
	auto pteaddr
	0 pteaddr!

	auto offset
	vaddr@ POOLSPACE - PAGESHIFT >> offset!

	auto pages
	0 pages!

	// find the length of the allocation by counting the number of clear bits
	// until the next bit in the end bitmap. we don't wrap this in any
	// synchronization mechanism because it shouldn't(?) be possible for
	// anybody to interfere with this range of the bitmap while it is
	// allocated, even if they (non-atomically) set bits in the same byte of
	// it.

	// XXX review this assumption for architectures like Alpha which don't
	// have atomic loads and stores of bytes. this also may have weird
	// interactions with some SMP schemes.

	auto bmph
	MiPoolSpacePagedEndBitmapHeader bmph!

	auto off
	offset@ off!

	while (off@ bmph@ ComBitmapBitGet ~~)
		1 off +=
		1 pages +=
	end

	1 pages +=

	// clear the final page bit.

	auto ipl
	IPLDPC KeIPLRaise ipl!

	1 // runlength
	off@ // index
	MiPoolSpacePagedEndBitmapHeader // header
	ComBitmapClearBits

	// trim the valid pages from the system working set.

	vaddr@ // startva
	vaddr@ pages@ PAGESHIFT << + // endva
	PsSystemProcess@ // process
	MiWorkingSetTrimRange

	ipl@ KeIPLLower

	auto count
	pages@ count!

	while (count@)
		if (vaddr@ PERPAGETABLEOFFSETMASK & ~~ pteaddr@ ~~ ||)
			vaddr@ // vaddr
			MmVirtualtoPTEAddress pteaddr!
		end

		if (pteaddr@@ PTE_KERNEL_DEMANDZERO ~=)
			0 // deletepte
			pteaddr@ // pteaddr
			vaddr@ // vaddr
			PsSystemProcess@ // process
			MiAnonymousPageDeleteByPTE
		end else
			PTE_KERNEL_ZERO pteaddr@!
		end

		1 count -=
		PTESIZE pteaddr +=
		PAGESIZE vaddr +=
	end

	pages@ // pages
	offset@ // offset
	MiPoolSpaceRelease

	pages@ PAGESHIFT << // charge
	MmQuotaSystem // quotablock
	MmQuotaBlockUnchargeVM

	if (noaccount@ ~~)
		auto rs
		HALCPUInterruptDisable rs!
		pages@ PAGESHIFT << MmPagedPoolBytesUsed -=
		rs@ HALCPUInterruptRestore
	end
end

fn MiNonpagedPoolAllocPages { bytes tag flags -- firstpfdbe ptr ok }
	auto pagesneeded
	bytes@ PAGEOFFSETMASK + PAGESHIFT >> pagesneeded!

	auto offset

	flags@ // pri
	pagesneeded@ // pagesneeded
	MiPoolSpaceReserve ok! offset!

	if (ok@)
		return
	end

	flags@ MUSTSUCCEED | // pri
	pagesneeded@ // pages
	MmChargeCommit ok!

	if (ok@)
		pagesneeded@ // pages
		offset@ // offset
		MiPoolSpaceRelease

		return
	end

	if (flags@ 0xFF & ZEROMUST ~=)
		FREEFIRST flags |=
	end

	// the vaddrs are marked allocated, so we don't need to be in IPLDPC now,
	// since nobody else is going to touch this range.

	auto pfdbe

	auto vaddr
	offset@ PAGESHIFT << POOLSPACE + vaddr!

	vaddr@ ptr!

	auto i
	0 i!

	0 firstpfdbe!

	while (i@ pagesneeded@ <)
		auto phyaddr

		flags@ // priority
		MmPageAlloc ok! phyaddr! pfdbe!

		if (ok@)
			// failed to allocate... gotta go back and free/unmap those pages

			while (i@)
				PAGESIZE vaddr -=
				1 i -=

				0 // phyaddr
				0 // flags
				vaddr@ // vaddr
				MiPTEUpdateByVirtual drop phyaddr!

				phyaddr@ PAGESHIFT >> MiPageFree
			end

			pagesneeded@ MmUnchargeCommit

			pagesneeded@ // pages
			offset@ // offset
			MiPoolSpaceRelease

			return
		end

		if (firstpfdbe@ ~~)
			pfdbe@ firstpfdbe!
		end

		phyaddr@ PAGESHIFT << // phyaddr
		PTE_W PTE_V | // flags
		vaddr@ // vaddr
		MiPTEUpdateByVirtual drop drop

		tag@ pfdbe@ MiPageFrameEntryPool_Tag + !
		pagesneeded@ PAGESHIFT << pfdbe@ MiPageFrameEntryPool_ByteSize + !
		vaddr@ pfdbe@ MiPageFrameEntryPool_VirtualAddress + !
		0 pfdbe@ MiPageFrameEntryPool_ZeroIfNonPaged + !
		-1 pfdbe@ MiPageFrameEntryPool_Level + !

		PAGESIZE vaddr +=
		1 i +=
	end

	if (flags@ POOLEXP & ~~)
		auto rs
		HALCPUInterruptDisable rs!

		pagesneeded@ PAGESHIFT << MmNonpagedPoolBytesUsed +=

		if (MmNonpagedPoolBytesUsed@ MmNonpagedPoolBytesPeak@ >)
			MmNonpagedPoolBytesUsed@ MmNonpagedPoolBytesPeak!
		end

		rs@ HALCPUInterruptRestore
	end

	auto ipl
	IPLDPC KeIPLRaise ipl!

	pagesneeded@ MiPhysicalUsage +=

	firstpfdbe@ MiPoolPageInsert

	ipl@ KeIPLLower
end

fn MiNonpagedPoolFreePages { noaccount ptr -- }
	if (DEBUGCHECKS)
		if (ptr@ POOLSPACE <)
			ptr@ "MmPoolPageAlignedFree: ptr 0x%08x < POOLSPACE\n" KeCrash
		end

		if (ptr@ POOLSPACE POOLSIZE + >=)
			ptr@ "MmPoolPageAlignedFree: ptr 0x%08x beyond pool space\n" KeCrash
		end

		if (ptr@ PAGEOFFSETMASK &)
			ptr@ "MmPoolPageAlignedFree: ptr 0x%08x not aligned\n" KeCrash
		end
	end

	auto offset
	ptr@ POOLSPACE - PAGESHIFT >> offset!

	auto pfdbe
	ptr@ MmPoolPageGetPhysical pfdbe! drop

	auto ipl
	IPLDPC KeIPLRaise ipl!
	pfdbe@ MiPoolPageRemove
	ipl@ KeIPLLower

	auto pages
	pfdbe@ MiPageFrameEntryPool_ByteSize + @ PAGEOFFSETMASK + PAGESHIFT >> pages!

	auto i
	0 i!

	while (i@ pages@ <)
		auto phyaddr
		auto ok

		0 // phyaddr
		0 // flags
		ptr@ // vaddr
		MiPTEUpdateByVirtual drop phyaddr!

		phyaddr@ PAGESHIFT >> MiPageFree

		PAGESIZE ptr +=
		1 i +=
	end

	auto bmpheader
	MiPoolSpaceBitmapHeader bmpheader!

	auto rs
	HALCPUInterruptDisable rs!

	if (noaccount@ ~~)
		pages@ PAGESHIFT << MmNonpagedPoolBytesUsed -=
	end

	pages@ MiPhysicalUsage -=

	rs@ HALCPUInterruptRestore

	pages@ MmUnchargeCommit

	pages@ // pages
	offset@ // offset
	MiPoolSpaceRelease
end

fn MmChargeBytesGet { bytes -- charge }
	// round up to nearest long
	bytes@ 3 + 3 ~ & bytes!

	if (bytes@ MiAllocatedHeapBlock_SIZEOF + PAGESIZE 2 / >=)
		bytes@ PAGEOFFSETMASK + PAGENUMBERMASK & charge!
	end else
		bytes@ MmHeapChargeBytesGet charge!
	end
end

fn MmBlockChargeGet { block -- charge }
	if (DEBUGCHECKS)
		if (block@ 3 &)
			block@ "MmBlockChargeGet: ptr 0x%08x not aligned\n" KeCrash
		end
	end

	if (block@ PAGEOFFSETMASK & ~~)
		if (DEBUGCHECKS)
			if (block@ POOLSPACE <)
				block@ "MmBlockChargeGet: ptr 0x%08x < POOLSPACE\n" KeCrash
			end

			if (block@ POOLSPACE POOLSIZE + >=)
				block@ "MmBlockChargeGet: ptr 0x%08x beyond pool space\n" KeCrash
			end
		end

		// page aligned.
		// determine if the given block is in paged or nonpaged pool.

		auto pteaddr
		block@ // vaddr
		MmVirtualtoPTEAddress pteaddr!

		auto ipl
		IPLDPC KeIPLRaise ipl!

		auto ok
		auto phyaddr
		pteaddr@ // pteaddr
		MiPTEInterpret ok! drop phyaddr!

		if (ok@ ~~)
			// maybe paged or nonpaged. check PFDBE.

			auto pfdbe
			phyaddr@ PAGESHIFT >> MiPageFrameEntry_SIZEOF * MiPageFrameDatabase@ + pfdbe!

			if (pfdbe@ MiPageFrameEntryPool_ZeroIfNonPaged + @ 0 ==)
				ipl@ KeIPLLower

				// nonpaged.

				pfdbe@ MiPageFrameEntryPool_ByteSize + @ charge!

				return
			end
		end

		ipl@ KeIPLLower

		// paged.

		auto offset
		block@ POOLSPACE - PAGESHIFT >> offset!

		auto pages
		0 pages!

		auto bmph
		MiPoolSpacePagedEndBitmapHeader bmph!

		while (offset@ bmph@ ComBitmapBitGet ~~)
			1 offset +=
			1 pages +=
		end

		1 pages +=

		pages@ PAGESHIFT << charge!

		return
	end

	// not page aligned. charge is defined by pool header.

	block@ MmHeapChargeGet charge!
end

fn MmPoolPageGetPhysical { ptr -- pfn pfdbe }
	if (DEBUGCHECKS)
		if (ptr@ POOLSPACE <)
			ptr@ "MmPoolPageGetPhysical: ptr 0x%08x < POOLSPACE\n" KeCrash
		end

		if (ptr@ POOLSPACE POOLSIZE + >=)
			ptr@ "MmPoolPageGetPhysical: ptr 0x%08x beyond pool space\n" KeCrash
		end

		if (ptr@ 3 &)
			ptr@ "MmPoolPageGetPhysical: ptr 0x%08x not aligned\n" KeCrash
		end
	end

	auto pteaddr
	auto phyaddr
	auto ok

	ptr@ // vaddr
	MmVirtualtoPTEAddress pteaddr!

	pteaddr@ // pteaddr
	MiPTEInterpret ok! drop phyaddr!

	if (DEBUGCHECKS)
		if (ok@)
			ok@ ptr@ "MmPoolPageGetPhysical: ptr %08x wasn't mapped 2 (%i)\n" KeCrash
		end
	end

	phyaddr@ PAGESHIFT >> pfn!

	pfn@ MiPageFrameEntry_SIZEOF * MiPageFrameDatabase@ + pfdbe!
end

fn MiPoolPageRemove { pfdbe -- }
	// assumes IPLDPC or equivalent

	auto ls
	pfdbe@ MiPageFrameEntryPool_PoolListPrev + @ ls!

	auto ns
	pfdbe@ MiPageFrameEntryPool_PoolListNext + @ ns!

	if (ls@)
		ns@ ls@ MiPageFrameEntryPool_PoolListNext + !
	end else
		ns@ MiPoolPageListHead!
	end

	if (ns@)
		ls@ ns@ MiPageFrameEntryPool_PoolListPrev + !
	end
end

fn MiPoolPageInsert { pfdbe -- }
	// assumes IPLDPC or equivalent

	auto h
	MiPoolPageListHead@ h!

	if (h@ ~~)
		0 pfdbe@ MiPageFrameEntryPool_PoolListNext + !
		0 pfdbe@ MiPageFrameEntryPool_PoolListPrev + !

		pfdbe@ MiPoolPageListHead!
	end else
		0 pfdbe@ MiPageFrameEntryPool_PoolListPrev + !

		h@ pfdbe@ MiPageFrameEntryPool_PoolListNext + !
		pfdbe@ h@ MiPageFrameEntryPool_PoolListPrev + !
		pfdbe@ MiPoolPageListHead!
	end
end

fn MiMapQuickPage { phyaddr -- vaddr }
	if (DEBUGCHECKS)
		if (KeIPLCurrentGet IPLDPC ~=)
			"MiMapQuickPage: ipl != IPLDPC\n" KeCrash
		end
	end

	MiQuickPage@ vaddr!

	phyaddr@ // phyaddr
	PTE_V PTE_W | // flags
	vaddr@ // vaddr
	MiPTEUpdateByVirtual drop drop
end

fn MiUnmapQuickPage { vaddr -- }
	if (DEBUGCHECKS)
		if (KeIPLCurrentGet IPLDPC ~=)
			"MiUnmapQuickPage: ipl != IPLDPC\n" KeCrash
		end
	end

	return
end