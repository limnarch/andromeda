//
// Implements the kernel pool allocation entrypoints, and the page-aligned
// allocator.
//

#include "<df>/dragonfruit.h"

#include "<inc>/HALLog.h"
#include "<inc>/HALCPU.h"
#include "<inc>/HALMap.h"
#include "<inc>/HALDebug.h"

#include "<inc>/Kernel.h"

#include "<inc>/Executive.h"

#include "<inc>/Memory.h"

#include "<inc>/IO.h"

#include "<inc>/Security.h"

#include "<inc>/Process.h"

#include "<ll>/OSDLL/OSStatus.h"

#include "../../Common/Common.h"

buffer MmPoolSpaceBitmapHeader ComBitmapHeader_SIZEOF
public MmPoolSpaceBitmapHeader

buffer MmPoolSpacePagedEndBitmapHeader ComBitmapHeader_SIZEOF
public MmPoolSpacePagedEndBitmapHeader

buffer MmPoolSpaceBitmap (POOLPAGES 7 + 3 >>)
buffer MmPoolSpacePagedEndBitmap (POOLPAGES 7 + 3 >>)

var MmNonpagedPoolBytesUsed 0
public MmNonpagedPoolBytesUsed

var MmPagedPoolBytesUsed 0
public MmPagedPoolBytesUsed

var MmNonpagedPoolBytesPeak 0
public MmNonpagedPoolBytesPeak

var MmPagedPoolBytesPeak 0
public MmPagedPoolBytesPeak

var MmPoolPageListHead 0

// The heap is managed within a 32MB region of kernel space called
// "pool space". Allocations less than a page size are handled by a slab
// allocator within pool pages. Allocations of a page size or more are handled
// by directly allocating pool pages.

extern MmHeapInit { -- }

fn MmPoolInit { -- }
	fnsection "INIT$text"

	POOLPAGES // sizeinbits
	MmPoolSpaceBitmap // data	
	MmPoolSpaceBitmapHeader // header
	ComBitmapInitialize

	POOLPAGES // sizeinbits
	MmPoolSpacePagedEndBitmap // data	
	MmPoolSpacePagedEndBitmapHeader // header
	ComBitmapInitialize

	MmHeapInit
end

fn MmHeapPrintTag { tag -- }
	auto shf
	32 shf!

	while (shf@)
		8 shf -=

		auto c
		tag@ shf@ >> 0xFF & c!

		if (c@ 0x80 & ~~ c@ 0x20 >= &&)
			c@ Putc
		end else
			'!' Putc
		end
	end
end

fn MmHeapCheck { -- }
	"MmHeapCheck: not implemented\n" KeCrash
end

fn MmHeapDumpBlockInfo { block -- }

end

extern MmHeapDumpPage { tag page -- usage }

fn MmPoolDump { tag -- usage }
	auto pfdbe
	MmPoolPageListHead@ pfdbe!

	0 usage!

	while (pfdbe@)
		if (pfdbe@ MmPageFrameEntryPool_Level + @ -1 ==)
			// page-aligned

			if (tag@ ~~ pfdbe@ MmPageFrameEntryPool_Tag + @ tag@ == ||)
				pfdbe@ MmPageFrameEntryPool_Tag + @ MmHeapPrintTag
				pfdbe@ MmPageFrameEntryPool_ByteSize + @
				pfdbe@ MmPageFrameEntryPool_VirtualAddress + @
				" %08x (%d bytes)\n" Printf

				pfdbe@ MmPageFrameEntryPool_ByteSize + @ usage +=
			end
		end else
			// heap

			tag@ // tag
			pfdbe@ MmPageFrameDatabase@ - MmPageFrameEntry_SIZEOF / PAGESHIFT << IDENTITYSPACE |
			MmHeapDumpPage usage +=
		end

		pfdbe@ MmPageFrameEntryPool_PoolListNext + @ pfdbe!
	end
end

fn MmAllocWithTag { bytes tag flags -- ptr ok }
	if (DEBUGCHECKS)
		if (MmInited@ ~~)
			"MmAllocWithTag: used before MmInit called\n" KeCrash
		end

		if (KeIPLCurrentGet IPLDPC >)
			"MmAllocWithTag: ipl > IPLDPC\n" KeCrash
		end
	end

	if (bytes@ ~~)
		STATUS_INVALID_ARGUMENT ok!
	end

	if (flags@ 0 ==)
		// can't block, so give this nonpaged allocation a small leg up
		POOLALLOC flags!
	end

	// round up to nearest long
	bytes@ 3 + 3 ~ & bytes!

	if (bytes@ MmAllocatedHeapBlock_SIZEOF + PAGESIZE MMHEAPBLOCKMINSIZE - >=)
		if (flags@ PAGED &)
			bytes@ // bytes
			tag@ // tag
			flags@ // flags
			MmPagedPoolAllocPages ok! ptr!
		end else
			bytes@ // bytes
			tag@ // tag
			flags@ // flags
			MmNonpagedPoolAllocPages ok! ptr!
		end

		return
	end

	bytes@ // bytes
	tag@ // tag
	flags@ // flags
	MmHeapAlloc ok! ptr!
end

fn MmFree { ptr -- }
	if (DEBUGCHECKS)
		if (MmInited@ ~~)
			"MmFree: used before MmInit called\n" KeCrash
		end

		if (ptr@ -1 ==)
			"MmFree: tried to free -1 pointer\n" KeCrash
		end

		if (KeIPLCurrentGet IPLDPC >)
			"MmFree: ipl > IPLDPC\n" KeCrash
		end
	end

	if (ptr@ MMLOWESTSYSTEMADDRESS <)
		ptr@ "MmFree: tried to free null pointer (%x)\n" KeCrash
	end

	if (ptr@ PAGEOFFSETMASK & ~~)
		// page aligned.
		// determine if the given block is in paged or nonpaged pool.

		auto pteaddr
		ptr@ // vaddr
		HALPlatformKernelPageDirectory@ // pagemap
		MmVirtualtoPTEAddress pteaddr!

		auto ipl
		IPLDPC KeIPLRaise ipl!

		auto ok
		auto phyaddr
		pteaddr@ // pteaddr
		MmPTEInterpret ok! drop phyaddr!

		if (ok@)
			ipl@ KeIPLLower

			// only paged pool can be non-resident.
			// and also mistaken frees.

			if (DEBUGCHECKS)
				if (ptr@ POOLSPACE <)
					ptr@ "MmFree: ptr 0x%08x < POOLSPACE\n" KeCrash
				end

				if (ptr@ POOLSPACE POOLSIZE + >=)
					ptr@ "MmFree: ptr 0x%08x beyond pool space\n" KeCrash
				end
			end

			0 // noaccount
			ptr@ // ptr
			MmPagedPoolFreePages
		end else
			// maybe paged or nonpaged. check PFDBE.

			auto pfdbe
			phyaddr@ PAGESHIFT >> MmPageFrameEntry_SIZEOF * MmPageFrameDatabase@ + pfdbe!

			if (pfdbe@ MmPageFrameEntryPool_ZeroIfNonPaged + @ 0 ==)
				ipl@ KeIPLLower

				// nonpaged.

				ptr@ MmNonpagedPoolFreePages
			end else
				ipl@ KeIPLLower

				// paged.

				0 // noaccount
				ptr@ // ptr
				MmPagedPoolFreePages
			end
		end

		return
	end

	ptr@ MmHeapFree
end

var MmPoolPageHint 0

var MmPoolSpaceUsed 0
public MmPoolSpaceUsed

fn MmPoolSpaceReserve { pagesneeded -- offset ok }
	auto bmpheader
	MmPoolSpaceBitmapHeader bmpheader!

	auto ipl
	IPLDPC KeIPLRaise ipl!

	MmPoolPageHint@ // hint
	pagesneeded@ // runlength
	bmpheader@ // header
	ComBitmapFindRun ok! offset!

	if (ok@)
		if (DEBUGCHECKS)
			bmpheader@ ComBitmapDump
			ok@ pagesneeded@ "\n\n%d (%i)\n" Printf

			POOLPAGES
			MmPoolSpaceUsed@
			"used: %d / %d\n" Printf

			MmPagedPoolBytesUsed@ PAGESHIFT >>
			"pagedpool: %d\n" Printf

			while (1) end
		end

		ipl@ KeIPLLower

		STATUS_NO_MEMORY ok!

		return
	end

	offset@ MmPoolPageHint!

	pagesneeded@ // runlength
	offset@ // index
	bmpheader@ // header
	ComBitmapSetBits

	pagesneeded@ MmPoolSpaceUsed +=

	ipl@ KeIPLLower
end

fn MmPoolSpaceRelease { pages offset -- }
	auto ipl
	IPLDPC KeIPLRaise ipl!

	pages@ // runlength
	offset@ // index
	MmPoolSpaceBitmapHeader // header
	ComBitmapClearBits

	pages@ MmPoolSpaceUsed -=

	ipl@ KeIPLLower
end

fn MmPagedPoolAllocPages { bytes tag flags -- realva ok }
	// this is a kernel mapping, so reserve POOLSPACE and use that as the
	// startva. it must be done like this because kernel page tables are
	// necessarily not dynamic.

	auto pages
	bytes@ PAGEOFFSETMASK + PAGESHIFT >> pages!

	pages@ PAGESHIFT << // charge
	MmQuotaSystem // quotablock
	MmQuotaBlockChargeVM ok!

	if (ok@)
		return
	end

	auto startva
	pages@ MmPoolSpaceReserve ok! startva!

	if (ok@)
		pages@ PAGESHIFT << // charge
		MmQuotaSystem // quotablock
		MmQuotaBlockUnchargeVM

		return
	end

	// set the bit for the final page in this allocation in the final page
	// bitmap so we can count bits to know how long it is when we go to free.

	auto ipl
	IPLDPC KeIPLRaise ipl!

	1 // runlength
	startva@ pages@ 1 - + // index
	MmPoolSpacePagedEndBitmapHeader // header
	ComBitmapSetBits

	if (flags@ PAGEPOOLEXP & ~~)
		pages@ PAGESHIFT << MmPagedPoolBytesUsed +=

		if (MmPagedPoolBytesUsed@ MmPagedPoolBytesPeak@ >)
			MmPagedPoolBytesUsed@ MmPagedPoolBytesPeak!
		end
	end

	ipl@ KeIPLLower

	// calculate the vaddr from the index MmPoolSpaceReserve gave us.

	startva@ PAGESHIFT << POOLSPACE + startva!
	startva@ realva!

	// initialize the PTEs in that region of POOLSPACE as kernel demand-
	// zero. this avoids having to allocate a VAD for private kernel
	// mappings, which makes them much cheaper. we can't do this for user-
	// space because userspace page tables are dynamically created and
	// deleted as pages are faulted in and removed which necessitates a
	// more permanent place for information. also, VADs are going to be
	// placed in paged pool, which will cause a dependency cycle if we use
	// them for private kernel mappings.

	auto pteaddr
	0 pteaddr!

	while (pages@)
		if (startva@ PERPAGETABLEOFFSETMASK & ~~ pteaddr@ ~~ ||)
			startva@ // vaddr
			HALPlatformKernelPageDirectory@ // pagemap
			MmVirtualtoPTEAddress pteaddr!
		end

		PTE_KERNEL_DEMANDZERO pteaddr@!

		4 pteaddr +=
		PAGESIZE startva +=
		1 pages -=
	end

	// TODO Tag
end

fn MmPagedPoolFreePages { noaccount vaddr -- }
	auto pteaddr
	0 pteaddr!

	auto offset
	vaddr@ POOLSPACE - PAGESHIFT >> offset!

	auto pages
	0 pages!

	// find the length of the allocation by counting the number of clear bits
	// until the next bit in the end bitmap. we don't wrap this in any
	// synchronization mechanism because it shouldn't(?) be possible for
	// anybody to interfere with this range of the bitmap while it is
	// allocated, even if they (non-atomically) set bits in the same byte of
	// it.

	// XXX review this assumption for architectures like Alpha which don't
	// have atomic loads and stores of bytes. this also may have weird
	// interactions with some SMP schemes.

	auto bmph
	MmPoolSpacePagedEndBitmapHeader bmph!

	auto off
	offset@ off!

	while (off@ bmph@ ComBitmapBitGet ~~)
		1 off +=
		1 pages +=
	end

	1 pages +=

	// clear the final page bit.

	auto ipl
	IPLDPC KeIPLRaise ipl!

	1 // runlength
	off@ // index
	MmPoolSpacePagedEndBitmapHeader // header
	ComBitmapClearBits

	ipl@ KeIPLLower

	// trim the valid pages from the system working set.

	KeThreadCurrent@ KeThreadIgnoreKill drop
	PsSystemProcess@ MmWorkingSetLock drop

	vaddr@ // startva
	vaddr@ pages@ PAGESHIFT << + // endva
	PsSystemProcess@ // process
	MmWorkingSetTrimRange

	PsSystemProcess@ MmWorkingSetUnlock
	KeThreadCurrent@ KeThreadAcceptKill drop

	auto count
	pages@ count!

	while (count@)
		if (vaddr@ PERPAGETABLEOFFSETMASK & ~~ pteaddr@ ~~ ||)
			vaddr@ // vaddr
			HALPlatformKernelPageDirectory@ // pagemap
			MmVirtualtoPTEAddress pteaddr!
		end

		IPLDPC KeIPLRaise ipl!

		auto pte
		pteaddr@@ pte!

		if (DEBUGCHECKS)
			if (pte@ PTE_V &)
				"MmPagedPoolFreePages: valid PTE found despite clearing working set\n" KeCrash
			end
		end

		if (pte@ PTE_INSWAP &)
			// free private page in swap

			ipl@ KeIPLLower

			pte@ IOSwapPageFreePTE
		end elseif (pte@ PTE_TRANSITION &)
			// free private page

			ipl@ // ipl
			pte@ PAGENUMBERMASK & PAGESHIFT >> MmPageFrameEntry_SIZEOF * MmPageFrameDatabase@ + // pfdbe
			MmAnonymousPageDelete
		end elseif (pte@ PTE_KERNEL_DEMANDZERO ==)
			ipl@ KeIPLLower
		end else
			pte@ "MmPagedPoolFreePages: strange PTE %08x\n" KeCrash
		end

		0 // phyaddr
		0 // flags
		vaddr@ // vaddr
		pteaddr@ // pteaddr
		0 // asid
		MmPTEUpdate drop drop drop

		1 count -=
		4 pteaddr +=
		PAGESIZE vaddr +=
	end

	pages@ // pages
	offset@ // offset
	MmPoolSpaceRelease

	pages@ PAGESHIFT << // charge
	MmQuotaSystem // quotablock
	MmQuotaBlockUnchargeVM

	if (noaccount@ ~~)
		auto rs
		HALCPUInterruptDisable rs!
		pages@ PAGESHIFT << MmPagedPoolBytesUsed -=
		rs@ HALCPUInterruptRestore
	end
end

fn MmNonpagedPoolAllocPages { bytes tag flags -- ptr ok }
	auto pagesneeded
	bytes@ PAGEOFFSETMASK + PAGESHIFT >> pagesneeded!

	auto offset

	pagesneeded@ // pagesneeded
	MmPoolSpaceReserve ok! offset!

	if (ok@)
		return
	end

	auto rs
	HALCPUInterruptDisable rs!
	pagesneeded@ MmPhysicalCommitUsage +=
	rs@ HALCPUInterruptRestore

	// the vaddrs are marked allocated, so we don't need to be in IPLDPC now,
	// since nobody else is going to touch this range.

	auto pfdbe

	auto vaddr
	offset@ PAGESHIFT << POOLSPACE + vaddr!

	vaddr@ ptr!

	auto kdir
	HALPlatformKernelPageDirectory@ kdir!

	auto i
	0 i!

	auto firstpfdbe
	0 firstpfdbe!

	while (i@ pagesneeded@ <)
		auto phyaddr

		FREEFIRST flags@ | // priority
		MmPageAlloc ok! phyaddr! pfdbe!

		if (ok@)
			// failed to allocate... gotta go back and free/unmap those pages

			while (i@)
				PAGESIZE vaddr -=
				1 i -=

				auto uok

				0 // phyaddr
				0 // flags
				vaddr@ // vaddr
				kdir@ // pagemap
				0 // asid
				MmPTEUpdateByVirtual uok! drop phyaddr!

				if (DEBUGCHECKS)
					if (uok@)
						"MmPoolPageAlignedAlloc: failed to unmap\n" KeCrash
					end
				end

				phyaddr@ PAGESHIFT >> MmPageFree
			end

			HALCPUInterruptDisable rs!
			pagesneeded@ MmPhysicalCommitUsage -=
			rs@ HALCPUInterruptRestore

			pagesneeded@ // pages
			offset@ // offset
			MmPoolSpaceRelease

			return
		end

		if (firstpfdbe@ ~~)
			pfdbe@ firstpfdbe!
		end

		phyaddr@ PAGESHIFT << // phyaddr
		PTE_G PTE_K | PTE_W | PTE_V | // flags
		vaddr@ // vaddr
		kdir@ // pagemap
		0 // asid
		MmPTEUpdateByVirtual ok! drop drop

		if (DEBUGCHECKS)
			if (ok@)
				"MmPoolPageAlignedAlloc: failed to map virtual address\n" KeCrash
			end
		end

		tag@ pfdbe@ MmPageFrameEntryPool_Tag + !
		pagesneeded@ PAGESHIFT << pfdbe@ MmPageFrameEntryPool_ByteSize + !
		vaddr@ pfdbe@ MmPageFrameEntryPool_VirtualAddress + !
		0 pfdbe@ MmPageFrameEntryPool_ZeroIfNonPaged + !
		-1 pfdbe@ MmPageFrameEntryPool_Level + !

		PAGESIZE vaddr +=
		1 i +=
	end

	HALCPUInterruptDisable rs!

	pagesneeded@ PAGESHIFT << MmNonpagedPoolBytesUsed +=

	if (MmNonpagedPoolBytesUsed@ MmNonpagedPoolBytesPeak@ >)
		MmNonpagedPoolBytesUsed@ MmNonpagedPoolBytesPeak!
	end

	rs@ HALCPUInterruptRestore

	auto ipl
	IPLDPC KeIPLRaise ipl!
	firstpfdbe@ MmPoolPageInsert
	ipl@ KeIPLLower

	firstpfdbe@ pfdbe!
end

fn MmNonpagedPoolFreePages { ptr -- }
	if (DEBUGCHECKS)
		if (ptr@ POOLSPACE <)
			ptr@ "MmPoolPageAlignedFree: ptr 0x%08x < POOLSPACE\n" KeCrash
		end

		if (ptr@ POOLSPACE POOLSIZE + >=)
			ptr@ "MmPoolPageAlignedFree: ptr 0x%08x beyond pool space\n" KeCrash
		end

		if (ptr@ 3 &)
			ptr@ "MmPoolPageAlignedFree: ptr 0x%08x not aligned\n" KeCrash
		end
	end

	auto offset
	ptr@ POOLSPACE - PAGESHIFT >> offset!

	auto pfdbe
	ptr@ MmPoolPageGetPhysical pfdbe! drop

	auto pages
	pfdbe@ MmPageFrameEntryPool_ByteSize + @ PAGEOFFSETMASK + PAGESHIFT >> pages!

	auto kdir
	HALPlatformKernelPageDirectory@ kdir!

	auto i
	0 i!

	while (i@ pages@ <)
		auto phyaddr
		auto ok

		0 // phyaddr
		0 // flags
		ptr@ // vaddr
		kdir@ // pagemap
		0 // asid
		MmPTEUpdateByVirtual ok! drop phyaddr!

		if (DEBUGCHECKS)
			if (ok@)
				"MmPoolPageAlignedFree: failed to unmap\n" KeCrash
			end
		end

		phyaddr@ PAGESHIFT >> MmPageFree

		PAGESIZE ptr +=
		1 i +=
	end

	auto bmpheader
	MmPoolSpaceBitmapHeader bmpheader!

	auto rs
	HALCPUInterruptDisable rs!
	pages@ PAGESHIFT << MmNonpagedPoolBytesUsed -=
	pages@ MmPhysicalCommitUsage -=
	rs@ HALCPUInterruptRestore

	pages@ // pages
	offset@ // offset
	MmPoolSpaceRelease

	auto ipl
	IPLDPC KeIPLRaise ipl!
	pfdbe@ MmPoolPageRemove
	ipl@ KeIPLLower
end

fn MmChargeBytesGet { bytes -- charge }
	// round up to nearest long
	bytes@ 3 + 3 ~ & bytes!

	if (bytes@ MmAllocatedHeapBlock_SIZEOF + PAGESIZE 2 / >=)
		bytes@ PAGEOFFSETMASK + PAGENUMBERMASK & charge!
	end else
		bytes@ MmHeapChargeBytesGet charge!
	end
end

fn MmBlockChargeGet { block -- charge }
	if (DEBUGCHECKS)
		if (block@ 3 &)
			block@ "MmBlockChargeGet: ptr 0x%08x not aligned\n" KeCrash
		end
	end

	if (block@ PAGEOFFSETMASK & ~~)
		if (DEBUGCHECKS)
			if (block@ POOLSPACE <)
				block@ "MmBlockChargeGet: ptr 0x%08x < POOLSPACE\n" KeCrash
			end

			if (block@ POOLSPACE POOLSIZE + >=)
				block@ "MmBlockChargeGet: ptr 0x%08x beyond pool space\n" KeCrash
			end
		end

		// page aligned.
		// determine if the given block is in paged or nonpaged pool.

		auto pteaddr
		block@ // vaddr
		HALPlatformKernelPageDirectory@ // pagemap
		MmVirtualtoPTEAddress pteaddr!

		auto ipl
		IPLDPC KeIPLRaise ipl!

		auto ok
		auto phyaddr
		pteaddr@ // pteaddr
		MmPTEInterpret ok! drop phyaddr!

		if (ok@ ~~)
			// maybe paged or nonpaged. check PFDBE.

			auto pfdbe
			phyaddr@ PAGESHIFT >> MmPageFrameEntry_SIZEOF * MmPageFrameDatabase@ + pfdbe!

			if (pfdbe@ MmPageFrameEntryPool_ZeroIfNonPaged + @ 0 ==)
				ipl@ KeIPLLower

				// nonpaged.

				pfdbe@ MmPageFrameEntryPool_ByteSize + @ charge!

				return
			end
		end

		ipl@ KeIPLLower

		// paged.

		auto offset
		block@ POOLSPACE - PAGESHIFT >> offset!

		auto pages
		0 pages!

		auto bmph
		MmPoolSpacePagedEndBitmapHeader bmph!

		while (offset@ bmph@ ComBitmapBitGet ~~)
			1 offset +=
			1 pages +=
		end

		1 pages +=

		pages@ PAGESHIFT << charge!

		return
	end

	// not page aligned. charge is defined by pool header.

	block@ MmHeapChargeGet charge!
end

fn MmPoolPageGetPhysical { ptr -- pfn pfdbe }
	if (DEBUGCHECKS)
		if (ptr@ POOLSPACE <)
			ptr@ "MmPoolPageGetPhysical: ptr 0x%08x < POOLSPACE\n" KeCrash
		end

		if (ptr@ POOLSPACE POOLSIZE + >=)
			ptr@ "MmPoolPageGetPhysical: ptr 0x%08x beyond pool space\n" KeCrash
		end

		if (ptr@ 3 &)
			ptr@ "MmPoolPageGetPhysical: ptr 0x%08x not aligned\n" KeCrash
		end
	end

	auto pteaddr
	auto phyaddr
	auto ok

	ptr@ // vaddr
	HALPlatformKernelPageDirectory@ // pagemap
	MmVirtualtoPTEAddress pteaddr!

	pteaddr@ // pteaddr
	MmPTEInterpret ok! drop phyaddr!

	if (DEBUGCHECKS)
		if (ok@)
			ok@ ptr@ "MmPoolPageGetPhysical: ptr %08x wasn't mapped 2 (%i)\n" KeCrash
		end
	end

	phyaddr@ PAGESHIFT >> pfn!

	pfn@ MmPageFrameEntry_SIZEOF * MmPageFrameDatabase@ + pfdbe!
end

fn MmPoolPageRemove { pfdbe -- }
	// assumes IPLDPC or equivalent

	auto ls
	pfdbe@ MmPageFrameEntryPool_PoolListPrev + @ ls!

	auto ns
	pfdbe@ MmPageFrameEntryPool_PoolListNext + @ ns!

	if (ls@)
		ns@ ls@ MmPageFrameEntryPool_PoolListNext + !
	end else
		ns@ MmPoolPageListHead!
	end

	if (ns@)
		ls@ ns@ MmPageFrameEntryPool_PoolListPrev + !
	end
end

fn MmPoolPageInsert { pfdbe -- }
	// assumes IPLDPC or equivalent

	auto h
	MmPoolPageListHead@ h!

	if (h@ ~~)
		0 pfdbe@ MmPageFrameEntryPool_PoolListNext + !
		0 pfdbe@ MmPageFrameEntryPool_PoolListPrev + !

		pfdbe@ MmPoolPageListHead!
	end else
		0 pfdbe@ MmPageFrameEntryPool_PoolListPrev + !

		h@ pfdbe@ MmPageFrameEntryPool_PoolListNext + !
		pfdbe@ h@ MmPageFrameEntryPool_PoolListPrev + !
		pfdbe@ MmPoolPageListHead!
	end
end