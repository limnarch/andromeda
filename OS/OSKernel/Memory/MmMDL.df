//
// Implements memory descriptor lists (MDLs) and buffer pinning, as well as
// mapping of user buffers into system space.
//

#include "<df>/dragonfruit.h"

#include "<inc>/HALLog.h"
#include "<inc>/HALCPU.h"
#include "<inc>/HALMap.h"
#include "<inc>/HALDebug.h"

#include "<inc>/Kernel.h"

#include "<inc>/Executive.h"

#include "<inc>/Memory.h"

#include "<inc>/IO.h"

#include "<inc>/Security.h"

#include "<inc>/Process.h"

#include "<ll>/OSDLL/OS.h"

fn MmMDLInitialize { mode length vaddr mdl -- }
	vaddr@ mdl@ MmMDLHeader_VirtualAddress + !
	length@ mdl@ MmMDLHeader_Length + !
	vaddr@ PAGEOFFSETMASK & length@ + PAGEOFFSETMASK + PAGESHIFT >> mdl@ MmMDLHeader_Pages + !
	0 mdl@ MmMDLHeader_Flags + !
	0 mdl@ MmMDLHeader_Process + !
	mode@ mdl@ MmMDLHeader_Mode + !

	0 mdl@ MmMDLHeader_PinCount + !
	0 mdl@ MmMDLHeader_MapCount + !

	0 mdl@ MmMDLHeader_MappedAddress + !
end

fn MmMDLAllocate { mode length vaddr kflags -- mdl ok }
	// try pretty hard not to call this since it puts IO integrity at the
	// mercy of the memory manager.
	//
	// it is sadly necessary for some stuff like non-contiguous disk transfers
	// from filesystem drivers.

	if (DEBUGCHECKS)
		if (KeIPLCurrentGet IPLDPC >=)
			"MmMDLAllocate: ipl >= IPLDPC\n" KeCrash
		end
	end

	auto bytes
	vaddr@ length@ MmMDLGetSize bytes!

	auto flags
	CANBLOCK flags!

	if (kflags@ IOKFLAG_SWAPOUT &)
		MUSTSUCCEED flags |=
	end elseif (kflags@ IOKFLAG_URGENT &)
		MUSTSUCCEEDL2 flags |=
	end

	bytes@ // bytes
	'MMDL' // tag
	flags@ // flags
	MmAllocWithTag ok! mdl!

	if (ok@)
		return
	end

	mode@ length@ vaddr@ mdl@ MmMDLInitialize
end

fn MmMDLGetSize { vaddr length -- size }
	// calculate the required size for an MDL to describe this buffer

	auto pages
	vaddr@ PAGEOFFSETMASK & length@ + PAGEOFFSETMASK + PAGESHIFT >> pages!

	pages@ 1 + 2 << MmMDLHeader_SIZEOF + size!
end

fn MmMDLSplit { vaddr length srcmdl destmdl -- }
	// split the source MDL into a destination MDL that only describes a
	// subset.

	auto srcva
	srcmdl@ MmMDLHeader_VirtualAddress + @ srcva!

	if (DEBUGCHECKS)
		if (srcmdl@ MmMDLHeader_PinCount + @ ~~)
			"MmMDLSplit: srcmdl must be pinned\n" KeCrash
		end

		// make sure the address and length actually fits into the source MDL
		if (vaddr@ srcva@ <)
			"MmMDLSplit: vaddr < srcmdl vaddr\n" KeCrash
		end

		if (vaddr@ length@ +
			srcva@ srcmdl@ MmMDLHeader_Length + @ + >)
			srcva@ srcmdl@ MmMDLHeader_Length + @ +
			vaddr@ length@ +
			"MmMDLSplit: vaddr+length > srcmdl vaddr+length (%d, %d)\n" KeCrash
		end
	end

	auto offset
	vaddr@ srcva@ - offset!

	auto pages
	vaddr@ PAGEOFFSETMASK & length@ + PAGEOFFSETMASK + PAGESHIFT >> pages!

	vaddr@ destmdl@ MmMDLHeader_VirtualAddress + !
	length@ destmdl@ MmMDLHeader_Length + !
	pages@ destmdl@ MmMDLHeader_Pages + !
	srcmdl@ MmMDLHeader_Flags + @ destmdl@ MmMDLHeader_Flags + !
	srcmdl@ MmMDLHeader_Process + @ destmdl@ MmMDLHeader_Process + !
	srcmdl@ MmMDLHeader_Mode + @ destmdl@ MmMDLHeader_Mode + !
	1 destmdl@ MmMDLHeader_PinCount + !
	srcmdl@ MmMDLHeader_MapCount + @ destmdl@ MmMDLHeader_MapCount + !

	// reuse the mapping from the source MDL
	srcmdl@ MmMDLHeader_MappedAddress + @ offset@ + destmdl@ MmMDLHeader_MappedAddress + !

	auto destmdlpos
	destmdl@ MmMDLHeader_SIZEOF + destmdlpos!

	auto srcmdlpos
	srcmdl@ MmMDLHeader_SIZEOF + offset@ PAGESHIFT >> 4 * + srcmdlpos!

	while (pages@)
		if (DEBUGCHECKS)
			if (srcmdlpos@@ -1 ==)
				"MmMDLSplit: srcmdl ended early\n" KeCrash
			end
		end

		srcmdlpos@@ destmdlpos@!

		4 destmdlpos +=
		4 srcmdlpos +=
		1 pages -=
	end
end

fn MmMDLFill { mdl -- }
	// fill up an MDL with system space.

	auto kdir
	HALPlatformKernelPageDirectory@ kdir!

	auto mdlpos
	mdl@ MmMDLHeader_SIZEOF + mdlpos!

	auto pages
	mdl@ MmMDLHeader_Pages + @ pages!

	auto buffer
	mdl@ MmMDLHeader_VirtualAddress + @ buffer!

	auto ok

	-1 mdlpos@!

	while (pages@)
		auto pte
		buffer@ // vaddr
		kdir@ // pagemap
		MmVirtualtoPTEAddress pte!

		auto pfdbe
		pte@ // pteaddr
		MmPTEInterpret ok! drop pfdbe!

		if (DEBUGCHECKS)
			if (ok@)
				"MmMDLFill: invalid system space\n" KeCrash
			end
		end

		pfdbe@ PAGESHIFT >> MmPageFrameEntry_SIZEOF * MmPageFrameDatabase@ + pfdbe!
		pfdbe@ mdlpos@!

		4 mdlpos +=
		-1 mdlpos@!

		1 pages -=
		PAGESIZE buffer +=
	end
end

var MmPinnedPagesLimit 0
public MmPinnedPagesLimit

var MmPinnedPagesCount 0
public MmPinnedPagesCount

fn MmMDLChargePinnedPages { count process -- ok }
	// charge the system for the number of pinned pages.
	// the usual consequence of failing this operation is that someone's IO
	// transfer will unexpectedly fail, so this needs to work reasonably.
	// the consequence of this operation being too lenient is that its easy to
	// put the system in a memory chokehold by accident.

	// will succeed if any of these is true:
	//   1. the process is owned by the root user,
	//   2. memory pressure is very low,
	//   3. the per-process guarantee can accomodate these pages,
	//   4. the total number of pinned pages in the system is below a
	//      certain threshold.

	STATUS_WS_QUOTA_EXCEEDED ok!

	auto ipl
	IPLDPC KeIPLRaise ipl!

	if (process@ PsProcess_UID + @ UID_SYSTEM ==)
		0 ok!
	end elseif (MmPageFreeCount@ MmEvictablePageCount@ + count@ + MmPageFreeCountSufficient@ >=)
		0 ok!
	end elseif (process@ PsProcess_PinnedPageCount + @ count@ + MMPROCESSPINGUARANTEE <)
		0 ok!
	end elseif (MmPinnedPagesCount@ count@ + MmPinnedPagesLimit@ <)
		0 ok!
	end

	if (ok@)
		ipl@ KeIPLLower

		return
	end

	count@ process@ PsProcess_PinnedPageCount + +=
	count@ MmPinnedPagesCount +=

	ipl@ KeIPLLower
end

fn MmMDLUnchargePinnedPages { count process -- }
	auto rs
	HALCPUInterruptDisable rs!

	count@ process@ PsProcess_PinnedPageCount + -=
	count@ MmPinnedPagesCount -=

	rs@ HALCPUInterruptRestore
end

fn MmMDLPin { lockforwrite mdl -- ok }
	// pin all of the pages described by the MDL into memory, resources
	// permitting. must be called in the context of the process whose buffer
	// it is.

	// NOTE: buffer parameters (start virtual address, length, etc) must have
	// been validated by the caller. this function trusts them blindly.

	if (mdl@ MmMDLHeader_PinCount + @)
		1 mdl@ MmMDLHeader_PinCount + +=
		0 ok!
		return
	end

	if (DEBUGCHECKS)
		if (mdl@ MmMDLHeader_Length + @ ~~)
			"MmMDLPin: length=0\n" KeCrash
		end

		if (mdl@ MmMDLHeader_Length + @ IOTRANSFERMAX >)
			"MmMDLPin: length>IOCHUNKMAX\n" KeCrash
		end
	end

	auto buffer
	mdl@ MmMDLHeader_VirtualAddress + @ buffer!

	if (buffer@ MMLOWESTSYSTEMADDRESS >=)
		mdl@ MmMDLFill

		// 2 so that nothing happens when it tries to unpin...
		2 mdl@ MmMDLHeader_PinCount + !
		0 ok!
		return
	end

	auto process
	KeProcessCurrent process!

	auto pages
	mdl@ MmMDLHeader_Pages + @ pages!

	// lock the process's VAD list mutex so we know nothing can get yoinked
	// out from underneath us by another thread in the process. in particular
	// this avoids race conditions involving the COW pages we might create
	// here if we probe for writing. it is fine to keep this mutex locked
	// during pagefaults we incur later because it can be locked recursively.

	process@ MmVADListLock ok!

	if (ok@)
		return
	end

	if (process@ PsProcess_MappedMMIOCount + @)
		// check to ensure that there aren't any VADs corresponding to memory-
		// -mapped devices overlapping our buffer, since these pages are yucky and
		// don't correspond to RAM and so they must not be looked up in the PFDB.

		mdl@ MmMDLHeader_VirtualAddress + @ // startva
		mdl@ MmMDLHeader_VirtualAddress + @ mdl@ MmMDLHeader_Length + @ + // endva
		process@ // process
		MmVADListCheckRange ok!

		if (ok@)
			process@ MmVADListUnlock

			return
		end
	end

	pages@ // count
	process@ // process
	MmMDLChargePinnedPages ok!

	if (ok@)
		process@ MmVADListUnlock

		return
	end

	process@ mdl@ MmMDLHeader_Process + !

	// touch all of the pages until their PTE is valid. call MmMDLUnpin to
	// clean up the partial pin if we fail at any point.

	auto mdlpos
	mdl@ MmMDLHeader_SIZEOF + mdlpos!

	1 mdl@ MmMDLHeader_PinCount + !

	if (lockforwrite@)
		MMMDL_MODIFIED mdl@ MmMDLHeader_Flags + |=
	end else
		MMMDL_MODIFIED ~ mdl@ MmMDLHeader_Flags + &=
	end

	-1 mdlpos@!

	while (pages@)
		if (lockforwrite@)
			// force any COWs and discover any read-only violations.

			buffer@ KeSafeProbeWrite ok!

			if (ok@)
				process@ MmVADListUnlock

				mdl@ MmMDLUnpin

				return
			end
		end

		auto pte

		auto ptpfdbe
		0 ptpfdbe!

		while (1)
			// poke the buffer page to make sure it is resident.
			buffer@ KeSafeGetByte ok! drop

			if (ok@)
				process@ MmVADListUnlock

				if (ptpfdbe@)
					ptpfdbe@ MmEvictablePageDereference drop
				end

				mdl@ MmMDLUnpin

				return
			end

			if (ptpfdbe@ ~~)
				0 // pri
				buffer@ // vaddr
				process@ KeProcess_PageDirectory + @ // pagemap
				MmPTEReference ok! pte! ptpfdbe!

				if (ok@)
					if (ok@ -1 ~=)
						process@ MmVADListUnlock

						mdl@ MmMDLUnpin

						return
					end

					0 ptpfdbe!

					continue
				end
			end

			auto ipl
			IPLDPC KeIPLRaise ipl!

			auto pfdbe
			pte@ // pteaddr
			MmPTEInterpret ok! drop pfdbe!

			if (ok@)
				// not resident, loop, re-poke, and try again.
				ipl@ KeIPLLower

				continue
			end

			pfdbe@ PAGESHIFT >> MmPageFrameEntry_SIZEOF * MmPageFrameDatabase@ + pfdbe!

			// we got a pfdbe, time to pin it and record it in our MDL.

			pfdbe@ MmEvictablePageReference drop

			ipl@ KeIPLLower

			pfdbe@ mdlpos@!
			-1 mdlpos@ 4 + !

			break
		end

		ptpfdbe@ MmEvictablePageDereference drop

		4 mdlpos +=
		1 pages -=
		PAGESIZE buffer +=

		// this is so that we probe at the start of each subsequent page after
		// the first one. the reason for this is that KeSafeProbeWrite works
		// by reading and then writing the same byte from the given address.
		// this is acceptable within the buffer because its contents
		// mid-syscall ought to be expected to be unpredictable anyway, but it
		// is not acceptable to do this to random memory that may be just
		// beyond the buffer because it will cause satanic race conditions
		// with any other threads in this process.
		PAGENUMBERMASK buffer &=
	end

	process@ MmVADListUnlock
end

fn MmMDLUnpin { mdl -- }
	// unpin all of the pages described by the MDL.

	if (DEBUGCHECKS)
		if (mdl@ MmMDLHeader_PinCount + @ ~~)
			"MmMDLUnpin: not pinned\n" KeCrash
		end
	end

	1 mdl@ MmMDLHeader_PinCount + -=

	if (mdl@ MmMDLHeader_PinCount + @)
		return
	end

	if (DEBUGCHECKS)
		if (mdl@ MmMDLHeader_MapCount + @)
			"MmMDLUnpin: mapped\n" KeCrash
		end
	end

	mdl@ MmMDLHeader_Pages + @ // count
	mdl@ MmMDLHeader_Process + @ // process
	MmMDLUnchargePinnedPages

	auto mdlpos
	mdl@ MmMDLHeader_SIZEOF + mdlpos!

	while (mdlpos@@ -1 ~=)
		auto pfdbe
		mdlpos@@ pfdbe!

		if (mdl@ MmMDLHeader_Flags + @ MMMDL_MODIFIED &)
			pfdbe@ // pfdbe
			mdl@ MmMDLHeader_Process + @ // process
			MmEvictablePageModify
		end

		pfdbe@ MmEvictablePageDereference drop

		4 mdlpos +=
	end
end

fn MmMDLMap { mdl -- ok }
	// map the page frames into system space that are pinned by the MDL.

	if (mdl@ MmMDLHeader_MapCount + @)
		1 mdl@ MmMDLHeader_MapCount + +=
		0 ok!
		return
	end

	if (mdl@ MmMDLHeader_VirtualAddress + @ MMLOWESTSYSTEMADDRESS >=)
		mdl@ MmMDLHeader_VirtualAddress + @ mdl@ MmMDLHeader_MappedAddress + !
		2 mdl@ MmMDLHeader_MapCount + !
		0 ok!
		return
	end

	auto pages
	mdl@ MmMDLHeader_Pages + @ pages!

	auto offset
	pages@ MmPoolSpaceReserve ok! offset!

	if (ok@)
		return
	end

	1 mdl@ MmMDLHeader_MapCount + !

	auto vaddr
	offset@ PAGESHIFT << POOLSPACE | vaddr!

	mdl@ MmMDLHeader_VirtualAddress + @ PAGEOFFSETMASK & vaddr@ + mdl@ MmMDLHeader_MappedAddress + !

	auto kdir
	HALPlatformKernelPageDirectory@ kdir!

	auto mdlpos
	mdl@ MmMDLHeader_SIZEOF + mdlpos!

	while (pages@)
		auto phyaddr
		mdlpos@@ MmPageFrameDatabase@ - MmPageFrameEntry_SIZEOF / PAGESHIFT << phyaddr!

		phyaddr@ // phyaddr
		PTE_G PTE_K | PTE_W | PTE_V | // flags
		vaddr@ // vaddr
		kdir@ // pagemap
		0 // asid
		MmPTEUpdateByVirtual ok! drop drop

		if (DEBUGCHECKS)
			if (ok@)
				"MmMDLMap: failed to map virtual address\n" KeCrash
			end
		end

		PAGESIZE vaddr +=
		4 mdlpos +=
		1 pages -=
	end
end

fn MmMDLUnmap { mdl -- }
	// unmap the page frames from system space that are pinned by the MDL.

	if (DEBUGCHECKS)
		if (mdl@ MmMDLHeader_MapCount + @ ~~)
			"MmMDLUnmap: not mapped\n" KeCrash
		end
	end

	1 mdl@ MmMDLHeader_MapCount + -=

	if (mdl@ MmMDLHeader_MapCount + @)
		return
	end

	auto pages
	mdl@ MmMDLHeader_Pages + @ pages!

	auto vaddr
	mdl@ MmMDLHeader_MappedAddress + @ PAGENUMBERMASK & vaddr!

	auto kdir
	HALPlatformKernelPageDirectory@ kdir!

	auto mdlpos
	mdl@ MmMDLHeader_SIZEOF + mdlpos!

	while (pages@)
		auto ok
		0 // phyaddr
		0 // flags
		vaddr@ // vaddr
		kdir@ // pagemap
		0 // asid
		MmPTEUpdateByVirtual ok! drop drop

		if (DEBUGCHECKS)
			if (ok@)
				"MmMDLUnmap: failed to unmap virtual address\n" KeCrash
			end
		end

		PAGESIZE vaddr +=
		4 mdlpos +=
		1 pages -=
	end

	mdl@ MmMDLHeader_Pages + @ // pages
	mdl@ MmMDLHeader_MappedAddress + @ POOLSPACE - PAGESHIFT >> // offset
	MmPoolSpaceRelease
end