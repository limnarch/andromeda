//
// Implements memory descriptor lists (MDLs) and buffer pinning, as well as
// mapping of user buffers into system space.
//

#include "<df>/dragonfruit.h"

#include "<inc>/HALLog.h"
#include "<inc>/HALCPU.h"
#include "<inc>/HALMap.h"
#include "<inc>/HALDebug.h"

#include "<inc>/Kernel.h"

#include "<inc>/Executive.h"

#include "<inc>/Memory.h"

#include "<inc>/IO.h"

#include "<inc>/Security.h"

#include "<inc>/Process.h"

#include "<ll>/OSDLL/OS.h"

fn MmMDLInitialize { length vaddr mdl -- }
	vaddr@ mdl@ MmMDLHeader_VirtualAddress + !
	length@ mdl@ MmMDLHeader_Length + !
	vaddr@ PAGEOFFSETMASK & length@ + PAGEOFFSETMASK + PAGESHIFT >> mdl@ MmMDLHeader_Pages + !
	0 mdl@ MmMDLHeader_Flags + !
	0 mdl@ MmMDLHeader_Process + !

	0 mdl@ MmMDLHeader_MappedAddress + !
end

fn MmMDLGetSize { vaddr length -- size }
	// calculate the required size for an MDL to describe this buffer

	auto pages
	vaddr@ PAGEOFFSETMASK & length@ + PAGEOFFSETMASK + PAGESHIFT >> pages!

	pages@ 1 + 2 << MmMDLHeader_SIZEOF + size!
end

var MmPinnedPagesLimit 0
public MmPinnedPagesLimit

var MmPinnedPagesCount 0
public MmPinnedPagesCount

fn MmMDLChargePinnedPages { count process -- ok }
	// charge the system for the number of pinned pages.
	// the unusual consequence of failing this operation is that someone's IO
	// transfer will unexpectedly fail, so this needs to work reasonably.
	// the consequence of this operation being too lenient is that its easy to
	// put the system in a memory chokehold by accident.

	// will succeed if any of these is true:
	//   1. the process is owned by the root user,
	//   2. memory pressure is very low,
	//   3. the per-process guarantee can accomodate these pages,
	//   4. the total number of pinned pages in the system is below a
	//      certain threshold.

	STATUS_WS_QUOTA_EXCEEDED ok!

	auto ipl
	IPLDPC KeIPLRaise ipl!

	if (process@ PsProcess_UID + @ UID_SYSTEM ==)
		0 ok!
	end elseif (MmPageFreeCount@ MmEvictablePageCount@ + count@ + MmPageFreeCountSufficient@ >=)
		0 ok!
	end elseif (process@ PsProcess_PinnedPageCount + @ count@ + MMPROCESSPINGUARANTEE <)
		0 ok!
	end elseif (MmPinnedPagesCount@ count@ + MmPinnedPagesLimit@ <)
		0 ok!
	end

	if (ok@)
		ipl@ KeIPLLower

		return
	end

	count@ process@ PsProcess_PinnedPageCount + +=
	count@ MmPinnedPagesCount +=

	ipl@ KeIPLLower
end

fn MmMDLUnchargePinnedPages { count process -- }
	auto rs
	HALCPUInterruptDisable rs!

	count@ process@ PsProcess_PinnedPageCount + -=
	count@ MmPinnedPagesCount -=

	rs@ HALCPUInterruptRestore
end

fn MmMDLPin { lockforwrite mdl -- ok }
	// pin all of the pages described by the MDL into memory, resources
	// permitting. must be called in the context of the process whose buffer
	// it is.

	// NOTE: buffer parameters (start virtual address, length, etc) must have
	// been validated by the caller. this function trusts them blindly.

	if (mdl@ MmMDLHeader_Flags + @ MMMDL_PINNED &)
		0 ok!
		return
	end

	if (DEBUGCHECKS)
		if (mdl@ MmMDLHeader_Length + @ ~~)
			"MmMDLPin: length=0\n" KeCrash
		end

		if (mdl@ MmMDLHeader_Length + @ IOCHUNKMAX >)
			"MmMDLPin: length>IOCHUNKMAX\n" KeCrash
		end
	end

	auto process
	KeProcessCurrent process!

	auto buffer
	mdl@ MmMDLHeader_VirtualAddress + @ buffer!

	auto pages
	mdl@ MmMDLHeader_Pages + @ pages!

	// lock the process's VAD list mutex so we know nothing can get yoinked
	// out from underneath us by another thread in the process. in particular
	// this avoids race conditions involving the COW pages we might create
	// here if we probe for writing. it is fine to keep this mutex locked
	// during pagefaults we incur later because it can be locked recursively.

	process@ MmVADListLock ok!

	if (ok@)
		return
	end

	if (process@ PsProcess_MappedMMIOCount + @)
		// check to ensure that there aren't any VADs corresponding to memory-
		// -mapped devices overlapping our buffer, since these pages are yucky and
		// don't correspond to RAM and so they must not be looked up in the PFDB.

		mdl@ MmMDLHeader_VirtualAddress + @ // startva
		mdl@ MmMDLHeader_VirtualAddress + @ mdl@ MmMDLHeader_Length + @ + // endva
		process@ // process
		MmVADListCheckRange ok!

		if (ok@)
			process@ MmVADListUnlock

			return
		end
	end

	pages@ // count
	process@ // process
	MmMDLChargePinnedPages ok!

	if (ok@)
		process@ MmVADListUnlock

		return
	end

	process@ mdl@ MmMDLHeader_Process + !

	// touch all of the pages until their PTE is valid. call MmMDLUnpin to
	// clean up the partial pin if we fail at any point.

	auto mdlpos
	mdl@ MmMDLHeader_SIZEOF + mdlpos!

	MMMDL_PINNED mdl@ MmMDLHeader_Flags + |=

	if (lockforwrite@)
		MMMDL_MODIFIED mdl@ MmMDLHeader_Flags + |=
	end else
		MMMDL_MODIFIED ~ mdl@ MmMDLHeader_Flags + &=
	end

	-1 mdlpos@!

	while (pages@)
		if (lockforwrite@)
			// force any COWs and discover any read-only violations.

			buffer@ KeSafeProbeWrite ok!

			if (ok@)
				process@ MmVADListUnlock

				mdl@ MmMDLUnpin

				return
			end
		end

		while (1)
			// poke the buffer page to make sure it is resident.
			buffer@ KeSafeGetByte ok! drop

			if (ok@)
				process@ MmVADListUnlock

				mdl@ MmMDLUnpin

				return
			end

			auto ipl
			IPLDPC KeIPLRaise ipl!

			auto pte
			buffer@ // vaddr
			process@ KeProcess_PageDirectory + @ // pagemap
			MmVirtualtoPTEAddress ok! pte!

			if (ok@)
				// not resident, loop, re-poke, and try again.
				ipl@ KeIPLLower

				continue
			end

			auto pfdbe
			pte@ // pteaddr
			MmPTEInterpret ok! drop pfdbe!

			if (ok@)
				// not resident, loop, re-poke, and try again.
				ipl@ KeIPLLower

				continue
			end

			pfdbe@ PAGESHIFT >> MmPageFrameEntry_SIZEOF * MmPageFrameDatabase@ + pfdbe!

			// we got a pfdbe, time to pin it and record it in our MDL.

			pfdbe@ MmEvictablePageReference drop

			ipl@ KeIPLLower

			pfdbe@ mdlpos@!
			-1 mdlpos@ 4 + !

			break
		end

		4 mdlpos +=
		1 pages -=
		PAGESIZE buffer +=

		// this is so that we probe at the start of each subsequent page after
		// the first one. the reason for this is that KeSafeProbeWrite works
		// by reading and then writing the same byte from the given address.
		// this is acceptable within the buffer because its contents
		// mid-syscall ought to be expected to be unpredictable anyway, but it
		// is not acceptable to do this to random memory that may be just
		// beyond the buffer because it will cause satanic race conditions
		// with any other threads in this process.
		PAGENUMBERMASK buffer &=
	end

	process@ MmVADListUnlock
end

fn MmMDLUnpin { mdl -- }
	// unpin all of the pages described by the MDL.

	if (DEBUGCHECKS)
		if (mdl@ MmMDLHeader_Flags + @ MMMDL_PINNED & ~~)
			"MmMDLUnpin: not pinned\n" KeCrash
		end

		if (mdl@ MmMDLHeader_Flags + @ MMMDL_MAPPED &)
			"MmMDLUnpin: mapped\n" KeCrash
		end
	end

	mdl@ MmMDLHeader_Pages + @ // count
	mdl@ MmMDLHeader_Process + @ // process
	MmMDLUnchargePinnedPages

	MMMDL_PINNED ~ mdl@ MmMDLHeader_Flags + &=

	auto mdlpos
	mdl@ MmMDLHeader_SIZEOF + mdlpos!

	while (mdlpos@@ -1 ~=)
		auto pfdbe
		mdlpos@@ pfdbe!

		if (mdl@ MmMDLHeader_Flags + @ MMMDL_MODIFIED &)
			pfdbe@ // pfdbe
			mdl@ MmMDLHeader_Process + @ // process
			MmEvictablePageModify
		end

		pfdbe@ MmEvictablePageDereference drop

		4 mdlpos +=
	end
end

fn MmMDLMap { mdl -- ok }
	// map the page frames into system space that are pinned by the MDL.

	if (mdl@ MmMDLHeader_Flags + @ MMMDL_MAPPED &)
		0 ok!
		return
	end

	auto pages
	mdl@ MmMDLHeader_Pages + @ pages!

	auto offset
	pages@ MmPoolSpaceReserve ok! offset!

	if (ok@)
		return
	end

	MMMDL_MAPPED mdl@ MmMDLHeader_Flags + |=

	auto vaddr
	offset@ PAGESHIFT << POOLSPACE | vaddr!

	vaddr@ mdl@ MmMDLHeader_MappedAddress + !

	auto kdir
	HALPlatformKernelPageDirectory@ kdir!

	auto mdlpos
	mdl@ MmMDLHeader_SIZEOF + mdlpos!

	while (pages@)
		auto phyaddr
		mdlpos@@ MmPageFrameDatabase@ - MmPageFrameEntry_SIZEOF / PAGESHIFT << phyaddr!

		phyaddr@ // phyaddr
		PTE_G PTE_K | PTE_W | PTE_V | // flags
		vaddr@ // vaddr
		kdir@ // pagemap
		0 // asid
		MmPTEUpdateByVirtual ok! drop drop

		if (DEBUGCHECKS)
			if (ok@)
				"MmMDLMap: failed to map virtual address\n" KeCrash
			end
		end

		PAGESIZE vaddr +=
		4 mdlpos +=
		1 pages -=
	end
end

fn MmMDLUnmap { mdl -- }
	// unmap the page frames from system space that are pinned by the MDL.

	if (DEBUGCHECKS)
		if (mdl@ MmMDLHeader_Flags + @ MMMDL_MAPPED & ~~)
			"MmMDLUnmap: not mapped\n" KeCrash
		end
	end

	MMMDL_MAPPED ~ mdl@ MmMDLHeader_Flags + &=

	auto pages
	mdl@ MmMDLHeader_Pages + @ pages!

	auto vaddr
	mdl@ MmMDLHeader_MappedAddress + @ vaddr!

	auto kdir
	HALPlatformKernelPageDirectory@ kdir!

	auto mdlpos
	mdl@ MmMDLHeader_SIZEOF + mdlpos!

	while (pages@)
		auto ok
		0 // phyaddr
		0 // flags
		vaddr@ // vaddr
		kdir@ // pagemap
		0 // asid
		MmPTEUpdateByVirtual ok! drop drop

		if (DEBUGCHECKS)
			if (ok@)
				"MmMDLUnmap: failed to unmap virtual address\n" KeCrash
			end
		end

		PAGESIZE vaddr +=
		4 mdlpos +=
		1 pages -=
	end

	mdl@ MmMDLHeader_Pages + @ // pages
	mdl@ MmMDLHeader_MappedAddress + @ POOLSPACE - PAGESHIFT >> // offset
	MmPoolSpaceRelease
end