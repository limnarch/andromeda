//
// Implements memory descriptor lists (MDLs) and buffer pinning, as well as
// mapping of user buffers into system space.
//

#include "<df>/dragonfruit.h"

#include "<inc>/HALLog.h"
#include "<inc>/HALCPU.h"
#include "<inc>/HALMap.h"
#include "<inc>/HALDebug.h"

#include "<inc>/Kernel.h"

#include "<inc>/Executive.h"

#include "<inc>/Memory.h"

#include "<ll>/OSDLL/OSStatus.h"

fn MmMDLInitialize { length vaddr mdl -- }
	vaddr@ mdl@ MmMDLHeader_VirtualAddress + !
	length@ mdl@ MmMDLHeader_Length + !

	0 mdl@ MmMDLHeader_Flags + !

	0 mdl@ MmMDLHeader_MappedAddress + !
end

var MmPinnedPagesCount 0

fn MmMDLPin { lockforwrite mdl -- ok }
	// pin all of the pages described by the MDL into memory, resources
	// permitting. must be called in the context of the process whose buffer
	// it is.

	// NOTE: buffer parameters (start virtual address, length, etc) must have
	// been validated by the caller. this function trusts them blindly.

	if (DEBUGCHECKS)
		if (mdl@ MmMDLHeader_Flags + @ MMMDL_PINNED &)
			"MmMDLPin: double pin\n" KeCrash
		end

		if (mdl@ MmMDLHeader_Length + @ ~~)
			"MmMDLPin: length=0\n" KeCrash
		end
	end

	auto process
	KeProcessCurrent process!

	auto buffer
	mdl@ MmMDLHeader_VirtualAddress + @ buffer!

	auto pages
	buffer@ PAGEOFFSETMASK & mdl@ MmMDLHeader_Length + @ + PAGEOFFSETMASK + PAGESHIFT >> pages!

	// lock the process's VAD list mutex so we know nothing can get yoinked
	// out from underneath us by another thread in the process. in particular
	// this avoids race conditions involving the COW pages we might create
	// here if we probe for writing. it is fine to keep this mutex locked
	// during pagefaults we incur later because it can be locked recursively.

	process@ MmVADListLock ok!

	if (ok@)
		return
	end

	// check to ensure that there aren't any VADs corresponding to memory-
	// -mapped devices overlapping our buffer, since these pages are yucky and
	// don't correspond to RAM and so they must not be looked up in the PFDB.

	mdl@ MmMDLHeader_VirtualAddress + @ // startva
	mdl@ MmMDLHeader_VirtualAddress + @ mdl@ MmMDLHeader_Length + @ + // endva
	process@ // process
	MmVADListCheckRange ok!

	if (ok@)
		process@ MmVADListUnlock

		return
	end

	// touch all of the pages until their PTE is valid. call MmMDLUnpin to
	// clean up the partial pin if we fail at any point.

	auto mdlpos
	mdl@ MmMDLHeader_SIZEOF + mdlpos!

	MMMDL_PINNED mdl@ MmMDLHeader_Flags + |=

	if (lockforwrite@)
		MMMDL_MODIFIED mdl@ MmMDLHeader_Flags + |=
	end

	-1 mdlpos@!

	while (pages@)
		if (lockforwrite@)
			// force any COWs and discover any read-only violations.

			buffer@ KeSafeProbeWrite ok!

			if (ok@)
				process@ MmVADListUnlock

				mdl@ MmMDLUnpin

				return
			end
		end

		while (1)
			buffer@ KeSafeGetByte ok! drop

			if (ok@)
				process@ MmVADListUnlock

				mdl@ MmMDLUnpin

				return
			end

			auto ipl
			IPLDPC KeIPLRaise ipl!

			auto pte
			buffer@ // vaddr
			process@ KeProcess_PageDirectory + @ // pagemap
			MmVirtualtoPTEAddress ok! pte!

			if (ok@)
				ipl@ KeIPLLower

				continue
			end

			auto pfdbe
			pte@ // pteaddr
			MmPTEInterpret ok! drop pfdbe!

			if (ok@)
				ipl@ KeIPLLower

				continue
			end

			pfdbe@ PAGESHIFT >> MmPageFrameEntry_SIZEOF * MmPageFrameDatabase@ + pfdbe!

			// we got a pfdbe, time to pin it and record it in our MDL.

			pfdbe@ MmEvictablePageReference drop

			ipl@ KeIPLLower

			pfdbe@ mdlpos@!
			-1 mdlpos@ 4 + !

			break
		end

		4 mdlpos +=
		1 pages -=
		PAGESIZE buffer +=

		// this is so that we probe at the start of each subsequent page after
		// the first one. the reason for this is that KeSafeProbeWrite works
		// by reading and then writing the same byte from the given address.
		// this is acceptable within the buffer because its contents
		// mid-syscall ought to be expected to be unpredictable anyway, but it
		// is not acceptable to do this to random memory that may be just
		// beyond the buffer because it will cause satanic race conditions
		// with any other threads in this process.

		PAGENUMBERMASK buffer &=
	end

	process@ MmVADListUnlock
end

fn MmMDLUnpin { mdl -- }
	// unpin all of the pages described by the MDL.

	if (DEBUGCHECKS)
		if (mdl@ MmMDLHeader_Flags + @ MMMDL_PINNED & ~~)
			"MmMDLUnpin: not pinned\n" KeCrash
		end
	end

	// TODO after unifying anon and file cache pages.
	// NOTE remember to modify pages if MDL had modify flag set
end

fn MmMDLMap { mdl -- vaddr ok }
	// map the page frames into system space that are pinned by the MDL.

	if (DEBUGCHECKS)
		if (mdl@ MmMDLHeader_Flags + @ MMMDL_MAPPED &)
			"MmMDLMap: double map\n" KeCrash
		end
	end

	// stub
end

fn MmMDLUnmap { mdl -- }
	// unmap the page frames from system space that are pinned by the MDL.

	if (DEBUGCHECKS)
		if (mdl@ MmMDLHeader_Flags + @ MMMDL_MAPPED & ~~)
			"MmMDLUnmap: not mapped\n" KeCrash
		end
	end

	// stub
end